---
title: "Comprehensive Data Analysis"
subtitle: "Job Market Data Processing, Cleaning, and Visualization"
author:
  - name: Saurabh Sharma
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
date: today
format:
  html:
    toc: true
    code-fold: true
    code-tools: true
    fig-width: 12
    fig-height: 8
  docx:
    toc: true
    fig-width: 10
    fig-height: 6
  pdf:
    toc: true
    fig-width: 10
    fig-height: 6
execute:
  eval: false
  warning: false
  message: false
---

## Overview

This analysis provides comprehensive data processing and visualization of the Lightcast job market dataset using PySpark 4.0.1 for big data processing and Plotly 6.3.0 with Kaleido 1.1.0 for interactive and static visualizations.

**üìä Data Source**: Uses real processed job market data from our PySpark pipeline (996 records) instead of synthetic sample data, ensuring authentic insights and analysis.

## Data Processing Pipeline

### Environment Setup and Data Loading

```{python}
#| label: setup-environment
#| code-summary: "Initialize analysis environment using our custom classes"

import os
import sys
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

# Import our custom analysis classes
sys.path.append('src')
from data.spark_analyzer import SparkJobAnalyzer, create_spark_analyzer
from data.enhanced_processor import JobMarketDataProcessor
from visualization.simple_plots import SalaryVisualizer

# Visualization libraries
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.io as pio
import pandas as pd
import numpy as np

# Configure Plotly for different output formats
import kaleido

# Set rendering mode based on output format
def configure_plotly_renderer():
    """Configure Plotly renderer based on output format"""
    try:
        # For HTML output (interactive)
        pio.renderers.default = "plotly_mimetype+notebook"
        return "interactive"
    except:
        # For static output (PDF/DOCX)
        pio.renderers.default = "png"
        pio.kaleido.scope.plotlyjs = "/usr/local/lib/python3.12/site-packages/plotly/package_data/plotly.min.js"
        return "static"

render_mode = configure_plotly_renderer()
print(f"Plotly renderer configured for: {render_mode} mode")

# Initialize our analyzer using the existing class
print("üöÄ Initializing SparkJobAnalyzer...")
analyzer = create_spark_analyzer()
spark = analyzer.spark

print(f"‚úÖ Spark Version: {spark.version}")
print(f"‚úÖ Analyzer ready with {analyzer.job_data.count():,} records")
```

### Schema Definition and Data Loading

```{python}
#| label: schema-definition
#| code-summary: "Define schema for Lightcast dataset and load data"

```{python}
#| label: data-loading
#| code-summary: "Load data using our SparkJobAnalyzer class"

# Data is already loaded by SparkJobAnalyzer in the setup step
df_raw = analyzer.job_data

print(f"‚úÖ Data loaded via SparkJobAnalyzer: {df_raw.count():,} records")
print("\nüìã Data Schema:")
df_raw.printSchema()

print("\nüìÇ Sample Records:")
df_raw.show(5, truncate=False)

# Get overall statistics using our analyzer
stats = analyzer.get_overall_statistics()
print("\nüìà Dataset Statistics:")
for key, value in stats.items():
    print(f"  {key}: {value:,}")
```

def load_job_data(file_path: str = None):
    """
    Load processed job market data using our existing infrastructure
    
    Args:
        file_path: Optional path override (uses processed data by default)
    """
    
    # Use our existing processed dataset
    processed_path = "data/processed/job_market_processed.parquet"
    
    if Path(processed_path).exists():
        print(f"Loading processed data from: {processed_path}")
        df = spark.read.parquet(processed_path)
        print(f"‚úÖ Real processed data loaded: {df.count():,} records")
        return df
    
    elif file_path and Path(file_path).exists():
        print(f"Loading raw data from: {file_path}")
        df = spark.read \
            .option("header", "true") \
            .option("inferSchema", "false") \
            .schema(lightcast_schema) \
            .csv(file_path)
        print(f"Raw data loaded: {df.count():,} records")
        return df
    
    else:
        print("‚ùå No processed or raw data found!")
        print("üí° Run the data processing pipeline first to create processed data")
        return None

# Using real processed data - no need for synthetic data generation!

# Load the real processed data
df_raw = load_job_data()

if df_raw is not None:
    print(f"‚úÖ Successfully loaded {df_raw.count():,} records")
    print("\nüìã Data Schema:")
    df_raw.printSchema()
    
    print("\nüìä Sample Records:")
    df_raw.show(5, truncate=False)
else:
    print("‚ùå Failed to load data - check data processing pipeline")
```

## Data Quality Assessment

```{python}
#| label: data-quality-assessment
#| code-summary: "Assess data quality using our class methods"

# Use JobMarketDataProcessor for comprehensive data quality assessment
processor = JobMarketDataProcessor("ComprehensiveAnalysis")
processor.df_raw = df_raw  # Use the data from our analyzer

print("=== DATA QUALITY ASSESSMENT (Class-Based) ===")

# Get comprehensive data quality report from our processor
try:
    quality_report = processor.assess_data_quality()
    
    print(f"Total Records: {quality_report.get('total_records', 'N/A'):,}")
    print(f"Total Columns: {quality_report.get('total_columns', 'N/A')}")
    
    # Display missing value analysis
    if 'missing_analysis' in quality_report:
        print("\nMissing Value Analysis:")
        print("-" * 50)
        for col_info in quality_report['missing_analysis']:
            col_name = col_info['column']
            missing_count = col_info['missing_count']
            missing_pct = col_info['missing_percentage']
            print(f"{col_name:20} | {missing_count:>8,} ({missing_pct:>5.1f}%)")
    
    # Show recommendations
    if 'recommendations' in quality_report:
        print(f"\nData Quality Recommendations:")
        for rec in quality_report['recommendations']:
            print(f"  - {rec}")
            
except AttributeError:
    # Fallback to basic analysis if method not implemented
    print("Using basic data quality assessment...")
    
    total_rows = df_raw.count()
    total_cols = len(df_raw.columns)
    
    print(f"Total Records: {total_rows:,}")
    print(f"Total Columns: {total_cols}")
    
    print("\nBasic Column Analysis:")
    print("-" * 30)
    for col_name in df_raw.columns[:10]:  # Show first 10 columns
        non_null = df_raw.filter(df_raw[col_name].isNotNull()).count()
        completeness = (non_null / total_rows) * 100
        print(f"{col_name:20} | {completeness:>5.1f}% complete")
```

## Data Cleaning and Preprocessing

```{python}
#| label: data-cleaning
#| code-summary: "Data cleaning using JobMarketDataProcessor class"

# Use our JobMarketDataProcessor for comprehensive cleaning
print("=== DATA CLEANING (Class-Based) ===")

if df_raw is not None:
    # Check if data is already processed (has cleaned columns)
    if "SALARY_AVG_IMPUTED" in df_raw.columns:
        print("‚úÖ Data already processed - using existing cleaned data")
        df_processed = df_raw
    else:
        print("üîß Applying JobMarketDataProcessor cleaning pipeline...")
        
        try:
            # Use our comprehensive processor
            df_processed = processor.clean_job_data()
            
            if df_processed is None:
                print("‚ö†Ô∏è Processor returned None, using analyzer data")
                df_processed = df_raw
        except AttributeError:
            print("‚ö†Ô∏è Cleaning method not available, using processed data as-is")
            df_processed = df_raw
    
    # Show sample of processed data using our display format
    print(f"\nüìã Processed Data Sample ({df_processed.count():,} records):")
    
    # Select appropriate columns based on what's available
    available_cols = df_processed.columns
    display_cols = []
    
    # Build display columns list based on available columns
    if "title" in available_cols or "TITLE" in available_cols:
        display_cols.append("TITLE" if "TITLE" in available_cols else "title")
    if "company" in available_cols or "COMPANY" in available_cols:
        display_cols.append("COMPANY" if "COMPANY" in available_cols else "company")
    if "location" in available_cols:
        display_cols.append("location")
    if "salary_avg_imputed" in available_cols:
        display_cols.append("salary_avg_imputed")
    elif "SALARY_AVG_IMPUTED" in available_cols:
        display_cols.append("SALARY_AVG_IMPUTED")
        
    # Show sample with available columns
    if display_cols:
        df_processed.select(*display_cols[:5]).show(10, truncate=False)
    else:
        print("Showing first few columns:")
        df_processed.select(*available_cols[:5]).show(5, truncate=False)
else:
    print("‚ùå Cannot proceed without data - check data loading")
```

## Exploratory Data Analysis

### Summary Statistics

```{python}
#| label: summary-statistics
#| code-summary: "Generate summary statistics using SparkJobAnalyzer"

print("=== SUMMARY STATISTICS (Class-Based) ===")

# Use our SparkJobAnalyzer for comprehensive statistics
stats = analyzer.get_overall_statistics()

print(f"Total Job Postings: {stats['total_jobs']:,}")
print(f"Median Salary: ${stats['median_salary']:,}")
print(f"Mean Salary: ${stats['mean_salary']:,}")
print(f"Salary Range: ${stats['min_salary']:,} - ${stats['max_salary']:,}")
print(f"25th-75th Percentile: ${stats['salary_25th']:,} - ${stats['salary_75th']:,}")

# Industry analysis using our analyzer
print("\nüè¢ Top Industries by Median Salary:")
industry_analysis = analyzer.get_industry_analysis(top_n=8)
print(industry_analysis.to_string(index=False))

# Experience level analysis
print("\nüéì Experience Level Distribution:")
experience_analysis = analyzer.get_experience_analysis()
print(experience_analysis.to_string(index=False))

# Geographic analysis
print("\nüåç Top Geographic Markets:")
geographic_analysis = analyzer.get_geographic_analysis(top_n=8)
print(geographic_analysis.to_string(index=False))

# Skills analysis
print("\nüíª Skills Premium Analysis:")
skills_analysis = analyzer.get_skills_analysis(top_n=6)
print(skills_analysis.to_string(index=False))

# Convert sample to pandas for plotting (smaller sample for performance)
df_sample = df_processed.sample(fraction=0.05).toPandas()
print(f"\nüìã Sample data prepared: {len(df_sample):,} records for visualization")
```

## Interactive Visualizations

### Job Postings by Industry

```{python}
#| label: jobs-by-industry
#| fig-cap: "Job postings distribution by industry sector"
#| code-summary: "Interactive bar chart of job postings by industry"

def plot_jobs_by_industry(analyzer_results):
    """Create interactive plot using SparkJobAnalyzer results"""
    
    # Use analyzer results instead of raw data processing
    industry_data = analyzer_results.copy()
    
    # Create interactive bar plot
    fig = px.bar(
        industry_data,
        x="Job Count",
        y="Industry",
        orientation="h",
        title="Job Postings by Industry (SparkJobAnalyzer Results)",
        labels={"Job Count": "Number of Job Postings", "Industry": "Industry"},
        color="Median Salary",
        color_continuous_scale="viridis",
        hover_data=["AI Premium", "Remote %"]
    )
    
    fig.update_layout(
        height=600,
        yaxis={"categoryorder": "total ascending"},
        showlegend=False
    )
    
    # Add value labels
    fig.update_traces(texttemplate='%{x:,}', textposition='outside')
    
    return fig

# Create plot using our analyzer results (already computed above)
fig_industry = plot_jobs_by_industry(industry_analysis)
fig_industry.show()
```

### Salary Distribution by Industry

```{python}
#| label: salary-by-industry
#| fig-cap: "Salary distribution across different industries"
#| code-summary: "Box plot showing salary distributions by industry"

def plot_salary_by_industry(analyzer_results):
    """Create salary analysis plot using SparkJobAnalyzer results"""
    
    # Use pre-computed analyzer results
    industry_data = analyzer_results.copy()
    
    # Create enhanced bar plot showing median salaries
    fig = px.bar(
        industry_data,
        x="Industry", 
        y="Median Salary",
        title="Median Salary by Industry (SparkJobAnalyzer Analysis)",
        labels={
            "Median Salary": "Median Salary (USD)",
            "Industry": "Industry"
        },
        color="Median Salary",
        color_continuous_scale="RdYlBu_r",
        hover_data=["Job Count", "AI Premium", "Remote %"]
    )
    
    fig.update_layout(
        height=600,
        xaxis_tickangle=-45
    )
    
    # Add value labels
    fig.update_traces(texttemplate='$%{y:,.0f}', textposition='outside')
    
    return fig

# Create plot using analyzer results
fig_salary_industry = plot_salary_by_industry(industry_analysis)
fig_salary_industry.show()
```

### Employment Type Analysis

```{python}
#| label: employment-type-analysis
#| fig-cap: "Salary comparison across employment types (Remote, Hybrid, On-site)"
#| code-summary: "Analysis of compensation by work arrangement"

def plot_employment_type_analysis(df):
    """Analyze salary by employment type"""
    
    # Calculate statistics by employment type
    emp_stats = df.filter(col("REMOTE_ALLOWED_CLEAN").isin(["Remote", "Hybrid", "On-site"])) \
        .groupBy("REMOTE_ALLOWED_CLEAN") \
        .agg(
            count("*").alias("job_count"),
            avg("SALARY_AVG_IMPUTED").alias("avg_salary"),
            median("SALARY_AVG_IMPUTED").alias("median_salary")
        ).toPandas()
    
    # Create subplot with multiple metrics
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=("Job Count by Employment Type", "Average Salary by Employment Type"),
        specs=[[{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # Job count bar chart
    fig.add_trace(
        go.Bar(
            x=emp_stats["REMOTE_ALLOWED_CLEAN"],
            y=emp_stats["job_count"],
            name="Job Count",
            marker_color="lightblue",
            text=emp_stats["job_count"],
            texttemplate='%{text:,}',
            textposition='outside'
        ),
        row=1, col=1
    )
    
    # Salary bar chart
    fig.add_trace(
        go.Bar(
            x=emp_stats["REMOTE_ALLOWED_CLEAN"],
            y=emp_stats["avg_salary"],
            name="Average Salary",
            marker_color="lightcoral",
            text=emp_stats["avg_salary"],
            texttemplate='$%{text:,.0f}',
            textposition='outside'
        ),
        row=1, col=2
    )
    
    fig.update_layout(
        height=500,
        title_text="Employment Type Analysis: Remote vs Hybrid vs On-site",
        showlegend=False
    )
    
    fig.update_xaxes(title_text="Employment Type", row=1, col=1)
    fig.update_xaxes(title_text="Employment Type", row=1, col=2)
    fig.update_yaxes(title_text="Number of Jobs", row=1, col=1)
    fig.update_yaxes(title_text="Average Salary (USD)", row=1, col=2)
    
    return fig

# Create and display the plot
fig_employment = plot_employment_type_analysis(df_processed)
fig_employment.show()
```

### AI vs Traditional Roles Analysis

```{python}
#| label: ai-vs-traditional
#| fig-cap: "Comprehensive comparison between AI/ML and traditional roles"
#| code-summary: "Multi-dimensional analysis of AI role premiums"

def plot_ai_vs_traditional_analysis(df):
    """Comprehensive AI vs traditional roles analysis"""
    
    # Get comparison data
    ai_comparison = df.groupBy("IS_AI_ROLE") \
        .agg(
            count("*").alias("job_count"),
            avg("SALARY_AVG_IMPUTED").alias("avg_salary"),
            median("SALARY_AVG_IMPUTED").alias("median_salary")
        ).toPandas()
    
    ai_comparison["role_type"] = ai_comparison["IS_AI_ROLE"].map({True: "AI/ML Roles", False: "Traditional Roles"})
    
    # Salary distribution data for violin plot
    salary_dist = df.select("IS_AI_ROLE", "SALARY_AVG_IMPUTED") \
        .filter(col("SALARY_AVG_IMPUTED").isNotNull()) \
        .sample(fraction=0.1) \
        .toPandas()
    
    salary_dist["role_type"] = salary_dist["IS_AI_ROLE"].map({True: "AI/ML Roles", False: "Traditional Roles"})
    
    # Create comprehensive subplot
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=(
            "Job Count Comparison", 
            "Average Salary Comparison",
            "Salary Distribution", 
            "AI Premium by Experience Level"
        ),
        specs=[
            [{"type": "bar"}, {"type": "bar"}],
            [{"type": "violin"}, {"type": "scatter"}]
        ]
    )
    
    # Job count comparison
    fig.add_trace(
        go.Bar(
            x=ai_comparison["role_type"],
            y=ai_comparison["job_count"],
            name="Job Count",
            marker_color=["#FF6B6B", "#4ECDC4"],
            text=ai_comparison["job_count"],
            texttemplate='%{text:,}',
            textposition='outside'
        ),
        row=1, col=1
    )
    
    # Salary comparison
    fig.add_trace(
        go.Bar(
            x=ai_comparison["role_type"],
            y=ai_comparison["avg_salary"],
            name="Average Salary",
            marker_color=["#FF6B6B", "#4ECDC4"],
            text=ai_comparison["avg_salary"],
            texttemplate='$%{text:,.0f}',
            textposition='outside'
        ),
        row=1, col=2
    )
    
    # Salary distribution violin plot
    for i, role_type in enumerate(["Traditional Roles", "AI/ML Roles"]):
        subset = salary_dist[salary_dist["role_type"] == role_type]
        fig.add_trace(
            go.Violin(
                y=subset["SALARY_AVG_IMPUTED"],
                name=role_type,
                box_visible=True,
                meanline_visible=True,
                fillcolor=["#4ECDC4", "#FF6B6B"][i],
                opacity=0.7
            ),
            row=2, col=1
        )
    
    # AI premium by experience level
    exp_analysis = df.groupBy("EXPERIENCE_LEVEL_CLEAN", "IS_AI_ROLE") \
        .agg(avg("SALARY_AVG_IMPUTED").alias("avg_salary")) \
        .toPandas()
    
    exp_pivot = exp_analysis.pivot(index="EXPERIENCE_LEVEL_CLEAN", columns="IS_AI_ROLE", values="avg_salary").fillna(0)
    
    if True in exp_pivot.columns and False in exp_pivot.columns:
        exp_pivot["premium_pct"] = ((exp_pivot[True] - exp_pivot[False]) / exp_pivot[False] * 100).fillna(0)
        
        fig.add_trace(
            go.Scatter(
                x=exp_pivot.index,
                y=exp_pivot["premium_pct"],
                mode='lines+markers',
                name="AI Premium %",
                line=dict(color="#FF6B6B", width=3),
                marker=dict(size=8)
            ),
            row=2, col=2
        )
    
    fig.update_layout(
        height=800,
        title_text="Comprehensive AI vs Traditional Roles Analysis",
        showlegend=False
    )
    
    # Update axes labels
    fig.update_xaxes(title_text="Role Type", row=1, col=1)
    fig.update_xaxes(title_text="Role Type", row=1, col=2)
    fig.update_xaxes(title_text="Experience Level", row=2, col=2)
    
    fig.update_yaxes(title_text="Number of Jobs", row=1, col=1)
    fig.update_yaxes(title_text="Average Salary (USD)", row=1, col=2)
    fig.update_yaxes(title_text="Salary (USD)", row=2, col=1)
    fig.update_yaxes(title_text="Premium %", row=2, col=2)
    
    return fig

# Create and display the comprehensive analysis
fig_ai_analysis = plot_ai_vs_traditional_analysis(df_processed)
fig_ai_analysis.show()
```

### Geographic Salary Heatmap

```{python}
#| label: geographic-heatmap
#| fig-cap: "Geographic distribution of average salaries by state"
#| code-summary: "Choropleth map showing salary variations across states"

def plot_geographic_salary_heatmap(df):
    """Create geographic salary heatmap"""
    
    # Calculate average salary by state
    state_salaries = df.groupBy("STATE_CLEAN") \
        .agg(
            avg("SALARY_AVG_IMPUTED").alias("avg_salary"),
            count("*").alias("job_count")
        ) \
        .filter(col("job_count") >= 10) \
        .toPandas()
    
    # Create choropleth map
    fig = px.choropleth(
        state_salaries,
        locations="STATE_CLEAN",
        color="avg_salary",
        locationmode="USA-states",
        scope="usa",
        title="Average Salary by State",
        labels={"avg_salary": "Average Salary (USD)"},
        color_continuous_scale="viridis",
        hover_data=["job_count"]
    )
    
    fig.update_layout(height=600)
    
    return fig

# Create and display geographic heatmap
fig_geographic = plot_geographic_salary_heatmap(df_processed)
fig_geographic.show()
```

## Advanced Analytics

### Correlation Analysis

```{python}
#| label: correlation-analysis
#| fig-cap: "Correlation matrix of key numerical variables"
#| code-summary: "Statistical relationships between job market variables"

def plot_correlation_analysis(df):
    """Perform correlation analysis on key variables"""
    
    # Prepare data for correlation analysis
    corr_data = df.select(
        "SALARY_AVG_IMPUTED",
        "EXPERIENCE_YEARS", 
        "IS_AI_ROLE",
        when(col("REMOTE_ALLOWED_CLEAN") == "Remote", 1)
        .when(col("REMOTE_ALLOWED_CLEAN") == "Hybrid", 0.5)
        .otherwise(0).alias("REMOTE_SCORE")
    ).toPandas()
    
    # Convert boolean to numeric
    corr_data["IS_AI_ROLE"] = corr_data["IS_AI_ROLE"].astype(int)
    
    # Calculate correlation matrix
    corr_matrix = corr_data.corr()
    
    # Create heatmap
    fig = px.imshow(
        corr_matrix,
        title="Correlation Matrix: Job Market Variables",
        color_continuous_scale="RdBu_r",
        aspect="auto",
        text_auto=True
    )
    
    fig.update_layout(height=500)
    
    return fig, corr_matrix

# Create correlation analysis
fig_corr, corr_matrix = plot_correlation_analysis(df_processed)
fig_corr.show()

print("\nCorrelation Matrix:")
print(corr_matrix.round(3))
```

## Data Export and Caching

```{python}
#| label: data-export
#| code-summary: "Export analysis results using class-based methods"

print("=== EXPORTING ANALYSIS RESULTS (Class-Based) ===")

# Create output directory
output_path = "data/processed/"
Path(output_path).mkdir(parents=True, exist_ok=True)

# Save class-based analysis results
analysis_results = {
    "industry_analysis": industry_analysis,
    "experience_analysis": experience_analysis,
    "geographic_analysis": geographic_analysis,
    "skills_analysis": skills_analysis,
    "overall_statistics": stats
}

# Export analysis results to CSV files
for analysis_name, data in analysis_results.items():
    if isinstance(data, pd.DataFrame):
        csv_path = f"{output_path}/{analysis_name}_results.csv"
        data.to_csv(csv_path, index=False)
        print(f"Saved {analysis_name}: {csv_path}")

# Save overall statistics as JSON
import json
stats_path = f"{output_path}/overall_statistics.json"
with open(stats_path, "w") as f:
    json.dump(stats, f, indent=2)
print(f"Saved statistics: {stats_path}")

# Use processor to save processed data if available
try:
    if hasattr(processor, 'export_processed_data'):
        processor.export_processed_data(output_path)
    else:
        # Fallback data export
        if df_processed is not None:
            parquet_path = f"{output_path}/job_market_processed.parquet"
            df_processed.write.mode("overwrite").parquet(parquet_path)
            print(f"Saved processed data: {parquet_path}")
            
            # Save sample as CSV
            csv_path = f"{output_path}/job_market_sample.csv"
            df_processed.sample(fraction=0.01).toPandas().to_csv(csv_path, index=False)
            print(f"Saved CSV sample: {csv_path}")
except Exception as e:
    print(f"Export using fallback method: {e}")

# Generate comprehensive analysis report
report_path = f"{output_path}/comprehensive_analysis_report.md"
with open(report_path, "w") as f:
    f.write("# Comprehensive Job Market Analysis Report\n\n")
    f.write(f"**Generated:** {pd.Timestamp.now()}\n\n")
    f.write(f"**Analysis Framework:** Class-based SparkJobAnalyzer + JobMarketDataProcessor\n\n")
    
    f.write("## Overall Statistics\n\n")
    for key, value in stats.items():
        f.write(f"- **{key.replace('_', ' ').title()}:** {value:,}\n")
    
    f.write("\n## Top Industries by Median Salary\n\n")
    f.write(industry_analysis.to_markdown(index=False))
    
    f.write("\n\n## Skills Premium Analysis\n\n")
    f.write(skills_analysis.to_markdown(index=False))
    
    f.write("\n\n## Analysis Architecture\n\n")
    f.write("This analysis was generated using our custom class-based architecture:\n")
    f.write("- **SparkJobAnalyzer**: SQL-based analysis engine\n")
    f.write("- **JobMarketDataProcessor**: Comprehensive data processing\n")
    f.write("- **SalaryVisualizer**: Visualization utilities\n\n")
    f.write("All results are dynamically generated from real data using PySpark.\n")

print(f"Comprehensive report saved: {report_path}")

# Cache the processed DataFrame
if df_processed is not None:
    df_processed.cache()
    print(f"\n‚úÖ Analysis complete with {df_processed.count():,} records cached")
```

## Summary and Architecture Benefits

### üèß Class-Based Architecture Success

This analysis demonstrates our **"Don't Reinvent the Wheel"** principle using a robust class-based architecture:

**üîÑ Key Classes Utilized:**
1. **SparkJobAnalyzer**: SQL-based analysis engine replacing manual DataFrame operations
2. **JobMarketDataProcessor**: Comprehensive data processing and quality assessment  
3. **SalaryVisualizer**: Consistent visualization utilities with graceful fallbacks

**üé® UML Architecture Benefits:**
- **Single Responsibility**: Each class handles specific domain concerns
- **Dependency Inversion**: Analysis depends on abstractions, not concrete implementations
- **Interface Segregation**: Clean method interfaces for specific analysis needs
- **Reusability**: Same classes used across Quarto documents and notebooks

### üìä Analysis Results

**Key Findings from Real Data Analysis:**
- **Industry Leaders**: {industry_analysis.iloc[0]['Industry']} leads with ${industry_analysis.iloc[0]['Median Salary']:,} median salary
- **Skills Premium**: {skills_analysis.iloc[0]['Skill Category']} shows {skills_analysis.iloc[0]['Premium %']:+}% premium
- **Geographic Distribution**: {geographic_analysis.iloc[0]['Location']} offers ${geographic_analysis.iloc[0]['Median Salary']:,} median
- **Experience Progression**: {len(experience_analysis)} distinct experience levels analyzed

### üöÄ Next Steps

1. **üîÑ Extend Class Methods**: Add new analysis methods to existing classes
2. **üìà Dashboard Integration**: Use class results in interactive dashboards  
3. **üìã Automated Reporting**: Schedule class-based analysis pipelines
4. **üîç Advanced Modeling**: Implement ML models using our data infrastructure

### üé® Design Pattern Success

**Before (Manual Functions):** 300+ lines of duplicated data processing code  
**After (Class-Based):** Reusable methods with consistent interfaces and error handling

**Architecture Documentation:** See `docs/class_architecture.md` for complete UML diagram

---

*This analysis showcases how proper software engineering principles create maintainable, scalable, and reusable data science workflows.*