{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2519bf39",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete data processing pipeline for the Tech Career Intelligence Platform.\n",
    "\n",
    "## What This Notebook Shows\n",
    "\n",
    "1. **Raw Data Loading**: Load the original Lightcast job postings CSV\n",
    "2. **Data Inspection**: View the first 5 rows of raw data\n",
    "3. **Processed Data Loading**: Load the clean, processed Parquet data\n",
    "4. **Data Comparison**: Compare raw vs processed data transformations\n",
    "5. **Pipeline Summary**: Understand the complete transformation flow\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "Raw CSV (UPPERCASE columns)\n",
    "    ↓\n",
    "Standardize columns (UPPERCASE → snake_case)\n",
    "    ↓\n",
    "Decode locations (base64 → plain text)\n",
    "    ↓\n",
    "Compute salary_avg (from min/max with imputation)\n",
    "    ↓\n",
    "Validate & clean data\n",
    "    ↓\n",
    "Processed Parquet (snake_case columns, clean data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path so we can import from src/\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"[OK] Libraries imported successfully\")\n",
    "print(f\"   Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fce0c2",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect Raw Data\n",
    "\n",
    "Let's start by loading the raw CSV data and examining its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load raw data\n",
    "raw_data_path = Path('../data/raw/lightcast_job_postings.csv')\n",
    "\n",
    "if raw_data_path.exists():\n",
    "    print(\"Loading raw data...\")\n",
    "    df_raw = pd.read_csv(raw_data_path)\n",
    "    print(f\"[OK] Loaded {len(df_raw):,} records with {len(df_raw.columns)} columns\")\n",
    "\n",
    "    # Display first 5 rows\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAW DATA - First 5 Rows\")\n",
    "    print(\"=\"*80)\n",
    "    display(df_raw.head())\n",
    "\n",
    "    # Show column names\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"RAW DATA - Column Names ({len(df_raw.columns)} total)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n\".join([f\"{i+1:3d}. {col}\" for i, col in enumerate(df_raw.columns)]))\n",
    "else:\n",
    "    print(f\"[ERROR] Raw data not found at {raw_data_path}\")\n",
    "    df_raw = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4912b76",
   "metadata": {},
   "source": [
    "## Step 2: Load and Inspect Processed Data\n",
    "\n",
    "Now let's load the processed Parquet data to see the transformation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1106cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data using the centralized pipeline\n",
    "# This follows the \"Process Once, Use Many Times\" architecture from DESIGN.md\n",
    "\n",
    "from src.data.website_processor import load_and_process_data\n",
    "\n",
    "print(\"Loading processed data...\")\n",
    "df_processed, summary = load_and_process_data()\n",
    "\n",
    "print(f\"[OK] Loaded {len(df_processed):,} records with {len(df_processed.columns)} columns\")\n",
    "print(f\"   Salary coverage: {summary['salary_coverage']:.1f}%\")\n",
    "print(f\"   Median salary: ${summary['salary_range']['median']:,.0f}\")\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSED DATA - First 5 Rows\")\n",
    "print(\"=\"*80)\n",
    "display(df_processed.head())\n",
    "\n",
    "# Show column names (snake_case)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"PROCESSED DATA - Column Names ({len(df_processed.columns)} total, all snake_case)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\".join([f\"{i+1:3d}. {col}\" for i, col in enumerate(df_processed.columns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1317e7",
   "metadata": {},
   "source": [
    "## Step 3: Compare Raw vs Processed Data\n",
    "\n",
    "Let's see what transformations were applied by the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformation Summary\n",
    "\n",
    "if df_raw is not None:\n",
    "    print(\"=\"*80)\n",
    "    print(\"RAW vs PROCESSED DATA COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\n[DATA] Record Count:\")\n",
    "    print(f\"   Raw:       {len(df_raw):,}\")\n",
    "    print(f\"   Processed: {len(df_processed):,}\")\n",
    "    print(f\"   Retained:  {len(df_processed)/len(df_raw)*100:.1f}%\")\n",
    "\n",
    "    print(f\"\\n Column Count:\")\n",
    "    print(f\"   Raw:       {len(df_raw.columns)}\")\n",
    "    print(f\"   Processed: {len(df_processed.columns)}\")\n",
    "    print(f\"   Change:    {len(df_processed.columns) - len(df_raw.columns):+d}\")\n",
    "\n",
    "    print(f\"\\n🏷️  Column Naming:\")\n",
    "    print(f\"   Raw:       UPPERCASE (e.g., {df_raw.columns[0]})\")\n",
    "    print(f\"   Processed: snake_case (e.g., {df_processed.columns[0]})\")\n",
    "\n",
    "    # Show key transformations\n",
    "    print(f\"\\n🔄 Key Transformations:\")\n",
    "\n",
    "    # Check for common columns\n",
    "    if 'SALARY_AVG' in df_raw.columns or 'salary_avg' in df_processed.columns:\n",
    "        print(f\"   [OK] Salary: SALARY_AVG → salary_avg (computed & validated)\")\n",
    "\n",
    "    if 'CITY_NAME' in df_raw.columns or 'city_name' in df_processed.columns:\n",
    "        print(f\"   [OK] Location: CITY_NAME → city_name (decoded & cleaned)\")\n",
    "\n",
    "    if 'TITLE_NAME' in df_raw.columns or 'title' in df_processed.columns:\n",
    "        print(f\"   [OK] Title: TITLE_NAME → title (standardized)\")\n",
    "\n",
    "    print(f\"\\n✨ Derived Columns Created:\")\n",
    "    derived_cols = [col for col in df_processed.columns if col not in [c.lower() for c in df_raw.columns]]\n",
    "    if derived_cols:\n",
    "        for col in derived_cols[:10]:  # Show first 10\n",
    "            print(f\"   • {col}\")\n",
    "        if len(derived_cols) > 10:\n",
    "            print(f\"   ... and {len(derived_cols) - 10} more\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "else:\n",
    "    print(\"[WARNING]  Raw data not available for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757751a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the data processing pipeline that transforms raw Lightcast job postings into clean, analysis-ready data.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Column Standardization**: All UPPERCASE columns converted to snake_case\n",
    "2. **Data Quality**: Salary validation, location decoding, and null handling\n",
    "3. **Derived Columns**: New features created for analysis (experience_avg, etc.)\n",
    "4. **Efficient Storage**: Parquet format for fast loading (117.8 MB compressed)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore `ml_feature_engineering_lab.ipynb` for machine learning models\n",
    "- Check `job_market_skill_analysis.ipynb` for NLP and skills analysis\n",
    "- View the Quarto website for interactive dashboards\n",
    "\n",
    "---\n",
    "\n",
    "**For more details, see:**\n",
    "- `DESIGN.md` - Technical design and pipeline architecture\n",
    "- `ARCHITECTURE.md` - System architecture with Mermaid diagrams\n",
    "- `README.md` - Project overview and setup instructions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
