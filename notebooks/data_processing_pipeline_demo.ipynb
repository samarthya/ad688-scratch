{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2519bf39",
   "metadata": {},
   "source": [
    "# Data Processing Pipeline Demo\n",
    "\n",
    "This notebook demonstrates the complete data processing pipeline for the Tech Career Intelligence Platform.\n",
    "\n",
    "## What This Notebook Shows\n",
    "\n",
    "1. **Raw Data Loading with PySpark**: Load the original Lightcast job postings CSV (72K rows)\n",
    "2. **Data Inspection**: View schema and sample data from PySpark DataFrame\n",
    "3. **Processed Data Loading with Pandas**: Load the clean, processed Parquet data\n",
    "4. **Data Comparison**: Compare raw vs processed data transformations\n",
    "5. **Pipeline Summary**: Understand the complete transformation flow\n",
    "\n",
    "## Technology Stack\n",
    "\n",
    "| Stage | Technology | Reason |\n",
    "|-------|-----------|--------|\n",
    "| **Raw Data ETL** | PySpark | Scalable for large CSV files (millions of rows) |\n",
    "| **Storage** | Parquet | Compressed, columnar, fast reads |\n",
    "| **Analysis** | Pandas | Rich API, fast for processed data (<100K rows) |\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "```\n",
    "Raw CSV (UPPERCASE columns, 72K rows)\n",
    " │\n",
    " ├─> PySpark ETL Pipeline:\n",
    " │   ├── Load with multiLine=True (robust CSV parsing)\n",
    " │   ├── Standardize columns (UPPERCASE → snake_case)\n",
    " │   ├── Decode locations (base64 → plain text)\n",
    " │   ├── Compute salary_avg (from salary_from/salary_to)\n",
    " │   ├── Clean employment/remote type data\n",
    " │   └── Validate & filter data\n",
    " │\n",
    " ├─> Save to Parquet (compressed, fast)\n",
    " │\n",
    " └─> Pandas Analysis (load processed Parquet)\n",
    "```\n",
    "\n",
    "## Key Architecture Decision\n",
    "\n",
    "**Why not use Pandas for raw data?**\n",
    "- Pandas loads entire CSV into memory (72K rows × 131 columns = high memory usage)\n",
    "- PySpark uses lazy evaluation (processes data on-demand)\n",
    "- PySpark scales to millions of rows without code changes\n",
    "- Same pipeline works locally or on a cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef9175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# Benign warnings: Ignoring them (TBD Will revist them post project)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PySpark imports (for large-scale ETL)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Pandas imports (for final processed data analysis only)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Add project root to path so we can import from src/\n",
    "project_root = Path().resolve().parent\n",
    "\n",
    "# Adding the project root to the path so we can import from src/\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"[OK] Libraries imported successfully\")\n",
    "print(f\"   Project root: {project_root}\")\n",
    "print(f\"   PySpark: For raw data ETL (72K rows)\")\n",
    "print(f\"   Pandas: For processed data analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fce0c2",
   "metadata": {},
   "source": [
    "## Step 1: Load and Inspect Raw Data with PySpark\n",
    "\n",
    "Let's start by loading the raw CSV data using **PySpark** for scalable data processing.\n",
    "\n",
    "**Why PySpark for Raw Data?**\n",
    "- **Large Dataset**: 72K rows × 131 columns (can scale to millions)\n",
    "- **Memory Efficient**: Lazy evaluation, distributed processing\n",
    "- **Production Ready**: Same code works on single machine or cluster\n",
    "- **Robust Parsing**: `multiLine=True` handles complex CSV formatting\n",
    "\n",
    "**Architecture Pattern**: \n",
    "```\n",
    "Raw CSV → PySpark ETL → Processed Parquet → Pandas Analysis\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facf3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Raw Data with PySpark (using project_root for absolute path)\n",
    "\n",
    "# Use project_root from previous cell (no relative paths!)\n",
    "raw_data_path = project_root / 'data' / 'raw' / 'lightcast_job_postings.csv'\n",
    "\n",
    "print(f\"[DATA] Raw data path: {raw_data_path}\")\n",
    "print(f\"[DATA] File exists: {raw_data_path.exists()}\")\n",
    "\n",
    "if raw_data_path.exists():\n",
    "    # Initialize Spark session for distributed processing\n",
    "    print(\"\\nInitializing Spark session...\")\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"RawDataInspection\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"[PYSPARK] Loading raw CSV with PySpark...\")\n",
    "    print(\"          Options: multiLine=True, escape='\\\"', header=True, inferSchema=True\")\n",
    "\n",
    "    # Load with PySpark (distributed, lazy evaluation)\n",
    "    spark_df_raw = spark.read.csv(\n",
    "        str(raw_data_path),\n",
    "        multiLine=True,\n",
    "        escape=\"\\\"\",\n",
    "        header=True,\n",
    "        inferSchema=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n[OK] Loaded {spark_df_raw.count():,} records with {len(spark_df_raw.columns)} columns\")\n",
    "    print(f\"     Memory: PySpark uses lazy evaluation - data processed on-demand\")\n",
    "    print(f\"     Technology: Distributed DataFrame (not Pandas)\")\n",
    "\n",
    "    # Display first 5 rows (PySpark DataFrame format)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAW DATA - First 5 Rows (PySpark DataFrame)\")\n",
    "    print(\"=\"*80)\n",
    "    spark_df_raw.show(5, truncate=50, vertical=False)\n",
    "\n",
    "    # Show column names (UPPERCASE from raw CSV)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"RAW DATA - Column Names ({len(spark_df_raw.columns)} total, UPPERCASE)\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\n\".join([f\"{i+1:3d}. {col}\" for i, col in enumerate(spark_df_raw.columns[:20])]))\n",
    "    print(f\"     ... and {len(spark_df_raw.columns) - 20} more columns\")\n",
    "\n",
    "    # Show schema (data types inferred by PySpark)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAW DATA - PySpark Schema (first 10 columns)\")\n",
    "    print(\"=\"*80)\n",
    "    for field in spark_df_raw.schema.fields[:10]:\n",
    "        print(f\"  {field.name:30s} {field.dataType}\")\n",
    "    print(f\"  ... and {len(spark_df_raw.columns) - 10} more columns\")\n",
    "\n",
    "    # Show data quality stats using PySpark\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAW DATA - PySpark Data Quality Check\")\n",
    "    print(\"=\"*80)\n",
    "    total_rows = spark_df_raw.count()\n",
    "    salary_rows = spark_df_raw.filter(col(\"SALARY_FROM\").isNotNull()).count()\n",
    "    print(f\"  Total rows: {total_rows:,}\")\n",
    "    print(f\"  Rows with salary: {salary_rows:,} ({salary_rows/total_rows*100:.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(f\"[ERROR] Raw data not found at {raw_data_path}\")\n",
    "    spark_df_raw = None\n",
    "    spark = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4912b76",
   "metadata": {},
   "source": [
    "## Step 2: Load and Inspect Processed Data\n",
    "\n",
    "Now let's load the processed Parquet data to see the transformation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1106cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data using the centralized pipeline\n",
    "# This follows the \"Process Once, Use Many Times\" architecture from DESIGN.md\n",
    "\n",
    "from src.data.website_processor import load_and_process_data\n",
    "\n",
    "print(\"Loading processed data...\")\n",
    "\n",
    "df_processed, summary = load_and_process_data()\n",
    "\n",
    "print(f\"[OK] Loaded {len(df_processed):,} records with {len(df_processed.columns)} columns\")\n",
    "print(f\" Salary coverage: {summary['salary_coverage']:.1f}%\")\n",
    "print(f\" Median salary: ${summary['salary_range']['median']:,.0f}\")\n",
    "\n",
    "# Display first 5 rows\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCESSED DATA - First 5 Rows\")\n",
    "print(\"=\"*80)\n",
    "display(df_processed.head())\n",
    "\n",
    "# Show column names (snake_case)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(f\"PROCESSED DATA - Column Names ({len(df_processed.columns)} total, all snake_case)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\".join([f\"{i+1:3d}. {col}\" for i, col in enumerate(df_processed.columns)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1317e7",
   "metadata": {},
   "source": [
    "## Step 3: Compare Raw vs Processed Data\n",
    "\n",
    "Let's see what transformations were applied by the data pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24b0da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Compare Raw (PySpark) vs Processed (Pandas) Data\n",
    "\n",
    "# Check if required variables exist and run comparison\n",
    "try:\n",
    "    # Test if variables are defined\n",
    "    _ = spark_df_raw\n",
    "    _ = df_processed\n",
    "    _ = spark\n",
    "\n",
    "    print(\"=\"*80)\n",
    "    print(\"RAW vs PROCESSED DATA COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Get counts from PySpark DataFrame\n",
    "    raw_count = spark_df_raw.count()\n",
    "    raw_columns = len(spark_df_raw.columns)\n",
    "\n",
    "    print(f\"\\n[DATA] Record Count:\")\n",
    "    print(f\"  Raw (PySpark): {raw_count:,}\")\n",
    "    print(f\"  Processed (Pandas): {len(df_processed):,}\")\n",
    "    print(f\"  Retained: {len(df_processed)/raw_count*100:.1f}%\")\n",
    "\n",
    "    print(f\"\\n[COLUMNS] Column Count:\")\n",
    "    print(f\"  Raw: {raw_columns}\")\n",
    "    print(f\"  Processed: {len(df_processed.columns)}\")\n",
    "    print(f\"  Change: {len(df_processed.columns) - raw_columns:+d} (salary_avg added)\")\n",
    "\n",
    "    print(f\"\\n[NAMING] Column Naming Convention:\")\n",
    "    print(f\"  Raw: UPPERCASE (e.g., {spark_df_raw.columns[0]})\")\n",
    "    print(f\"  Processed: snake_case (e.g., {df_processed.columns[0]})\")\n",
    "\n",
    "    # Show key transformations\n",
    "    print(f\"\\n[TRANSFORMS] Key Transformations:\")\n",
    "\n",
    "    # Check for salary column\n",
    "    if 'salary_avg' in df_processed.columns:\n",
    "        print(f\"  [OK] salary_avg: Computed from SALARY_FROM and SALARY_TO\")\n",
    "        print(f\"       Formula: (SALARY_FROM + SALARY_TO) / 2\")\n",
    "        print(f\"       Missing values: NOT imputed (maintains data integrity)\")\n",
    "\n",
    "    # Check for location decoding\n",
    "    if 'city_name' in df_processed.columns:\n",
    "        print(f\"  [OK] city_name: Decoded from base64 (if encoded)\")\n",
    "\n",
    "    # Check for employment/remote cleaning\n",
    "    if 'employment_type_name' in df_processed.columns:\n",
    "        print(f\"  [OK] employment_type_name: Special characters removed, nulls → 'Undefined'\")\n",
    "\n",
    "    if 'remote_type_name' in df_processed.columns:\n",
    "        print(f\"  [OK] remote_type_name: Missing values filled with 'Undefined'\")\n",
    "\n",
    "    # Show technology stack difference\n",
    "    print(f\"\\n[TECH] Technology Stack:\")\n",
    "    print(f\"  Raw data: PySpark DataFrame (distributed, lazy evaluation)\")\n",
    "    print(f\"  Processed data: Pandas DataFrame (in-memory, rich API)\")\n",
    "    print(f\"  Storage: Parquet (columnar, compressed)\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "    # Clean up Spark session\n",
    "    print(\"\\n[CLEANUP] Stopping Spark session to free memory...\")\n",
    "    spark.stop()\n",
    "    print(\"[OK] Spark session stopped\")\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"[ERROR] Missing required variable: {e}\")\n",
    "    print(\"[INFO] Please ensure you run cells in order:\")\n",
    "    print(\"       Step 1: Load raw data with PySpark\")\n",
    "    print(\"       Step 2: Load processed data with Pandas\")\n",
    "    print(\"       Step 3: Compare both datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757751a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates the data processing pipeline that transforms raw Lightcast job postings into clean, analysis-ready data.\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Column Standardization**: All UPPERCASE columns converted to snake_case\n",
    "2. **Data Quality**: Salary validation, location decoding, and null handling\n",
    "3. **Derived Columns**: New features created for analysis (experience_avg, etc.)\n",
    "4. **Efficient Storage**: Parquet format for fast loading (117.8 MB compressed)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore `ml_feature_engineering_lab.ipynb` for machine learning models\n",
    "- Check `job_market_skill_analysis.ipynb` for NLP and skills analysis\n",
    "- View the Quarto website for interactive dashboards\n",
    "\n",
    "---\n",
    "\n",
    "**For more details, see:**\n",
    "- `DESIGN.md` - Technical design and pipeline architecture\n",
    "- `ARCHITECTURE.md` - System architecture with Mermaid diagrams\n",
    "- `README.md` - Project overview and setup instructions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
