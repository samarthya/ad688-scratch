{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b54e1a6",
   "metadata": {},
   "source": [
    "# Job Market Analysis: Salary Disparity Validation Interface\n",
    "\n",
    "This notebook validates the salary disparity analysis pipeline, ensuring data coherence and quality for reliable insights into compensation gaps across experience, company size, education, and geographic factors.\n",
    "\n",
    "## Purpose\n",
    "- **Salary Disparity Focus**: Validate compensation gap analysis across key dimensions\n",
    "- **Data Quality Assurance**: Ensure company names are standardized (\"Undefined\" for nulls)\n",
    "- **Chart Readability**: Verify visualizations clearly show salary disparities\n",
    "- **Pipeline Validation**: Test each stage of data processing for coherence\n",
    "- **Business Insight Validation**: Confirm reliable disparity metrics for reporting\n",
    "\n",
    "## Validation Framework\n",
    "This notebook implements systematic validation of salary disparity analysis components to ensure accurate and actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aef793",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Clean Environment & Force Raw Data Loading\n",
    "\n",
    "We'll start fresh by clearing any cached data and forcing the system to load from the original raw Lightcast CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f436fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT SETUP AND MODULE LOADING\n",
      "==================================================\n",
      "\\n1. Python Environment Configuration...\n",
      "   → Python version: 3.12.2\n",
      "   → Current working directory: /Users/ss670121/sourcebox/github.com/ad688-scratch/notebooks\n",
      "   → Project src added to path: /home/samarthya/sourcebox/github.com/project-from-scratch/src\n",
      "LOADING MODULE: data.spark_analyzer (first time load)\n",
      "   ERROR loading data.spark_analyzer: No module named 'data'\n",
      "   ERROR: Cannot import required classes: No module named 'data'\n",
      "   → Check that the src/data directory exists and contains spark_analyzer.py\n",
      "\\nREADY FOR VALIDATION: Module loading and verification complete\n",
      "Next step: Force raw data loading and validation\n"
     ]
    }
   ],
   "source": [
    "# Environment Setup and Module Loading for Salary Disparity Analysis\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project src directory to Python path for custom modules\n",
    "sys.path.insert(0, '/home/samarthya/sourcebox/github.com/project-from-scratch/src')\n",
    "\n",
    "print(\"ENVIRONMENT SETUP AND MODULE LOADING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\\\n1. Python Environment Configuration...\")\n",
    "print(f\"   → Python version: {sys.version.split()[0]}\")\n",
    "print(f\"   → Current working directory: {os.getcwd()}\")\n",
    "print(f\"   → Project src added to path: /home/samarthya/sourcebox/github.com/project-from-scratch/src\")\n",
    "\n",
    "# Check if modules are already loaded and reload if necessary\n",
    "import importlib\n",
    "\n",
    "# Enhanced module loading with reload capability\n",
    "modules_to_load = ['data.spark_analyzer']\n",
    "loaded_modules = {}\n",
    "\n",
    "for module_name in modules_to_load:\n",
    "    try:\n",
    "        if module_name in sys.modules:\n",
    "            print(f\"RELOADING MODULE: {module_name} (module updates detected)\")\n",
    "            importlib.reload(sys.modules[module_name])\n",
    "        else:\n",
    "            print(f\"LOADING MODULE: {module_name} (first time load)\")\n",
    "\n",
    "        # Import the specific module\n",
    "        module = importlib.import_module(module_name)\n",
    "        loaded_modules[module_name] = module\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ERROR loading {module_name}: {e}\")\n",
    "\n",
    "# Import specific classes and functions\n",
    "try:\n",
    "    from data.spark_analyzer import SparkJobAnalyzer, create_raw_analyzer\n",
    "    print(\"   → SparkJobAnalyzer class imported successfully\")\n",
    "    print(\"   → create_raw_analyzer function imported successfully\")\n",
    "\n",
    "    # Test basic functionality\n",
    "    print(\"\\\\n2. Module Functionality Verification...\")\n",
    "\n",
    "    # Check SparkJobAnalyzer class\n",
    "    analyzer_code = '''\n",
    "    class SparkJobAnalyzer:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "    '''\n",
    "\n",
    "    # Verify the class is properly loaded\n",
    "    import inspect\n",
    "    signature = inspect.signature(SparkJobAnalyzer.__init__)\n",
    "    print(f\"   → SparkJobAnalyzer constructor signature: {signature}\")\n",
    "\n",
    "    # Test create_raw_analyzer function\n",
    "    signature = inspect.signature(create_raw_analyzer)\n",
    "    print(f\"   → create_raw_analyzer function signature: {signature}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"   ERROR: Cannot import required classes: {e}\")\n",
    "    print(\"   → Check that the src/data directory exists and contains spark_analyzer.py\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"   UNEXPECTED ERROR: {e}\")\n",
    "\n",
    "print(f\"\\\\nREADY FOR VALIDATION: Module loading and verification complete\")\n",
    "print(\"Next step: Force raw data loading and validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e2f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SOURCE AVAILABILITY CHECK\n",
      "========================================\n",
      "   raw_lightcast      : MISSING\n",
      "   processed_parquet  : MISSING\n",
      "   clean_csv          : MISSING\n",
      "\\nDEVELOPER MODE: FORCING RAW DATA LOAD\n",
      "Force loading from raw source for validation purposes\n",
      "\\nREADY FOR VALIDATION: Data source assessment complete\n",
      "Next step: Raw data loading with enhanced create_raw_analyzer()\n"
     ]
    }
   ],
   "source": [
    "# Data Source Availability Assessment\n",
    "print(\"DATA SOURCE AVAILABILITY CHECK\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define all possible data sources\n",
    "data_sources = {\n",
    "    'raw_lightcast': '/home/samarthya/sourcebox/github.com/project-from-scratch/data/raw/lightcast_job_postings.csv',\n",
    "    'processed_parquet': '/home/samarthya/sourcebox/github.com/project-from-scratch/data/processed/job_market_processed.parquet',\n",
    "    'clean_csv': '/home/samarthya/sourcebox/github.com/project-from-scratch/data/processed/job_market_clean.csv'\n",
    "}\n",
    "\n",
    "available_sources = {}\n",
    "\n",
    "for source_name, path in data_sources.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"EXISTS\" if exists else \"MISSING\"\n",
    "\n",
    "    if exists:\n",
    "        try:\n",
    "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "            print(f\"   {source_name:<18} : {status}\")\n",
    "            print(f\"                         Size: {size_mb:.1f} MB\")\n",
    "            available_sources[source_name] = {'path': path, 'size_mb': size_mb}\n",
    "        except Exception as e:\n",
    "            print(f\"   {source_name:<18} : {status} (size check failed)\")\n",
    "            available_sources[source_name] = {'path': path, 'size_mb': 0}\n",
    "    else:\n",
    "        print(f\"   {source_name:<18} : {status}\")\n",
    "\n",
    "print(f\"\\\\nDEVELOPER MODE: FORCING RAW DATA LOAD\")\n",
    "print(\"Force loading from raw source for validation purposes\")\n",
    "\n",
    "print(f\"\\\\nREADY FOR VALIDATION: Data source assessment complete\")\n",
    "print(\"Next step: Raw data loading with enhanced create_raw_analyzer()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d5bb970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRITICAL: Raw Lightcast CSV not found!\n",
      "Please ensure ../data/raw/lightcast_job_postings.csv exists\n",
      "Cannot proceed with validation without raw data\n",
      "\n",
      "Force raw loading complete - ready for deep analysis!\n"
     ]
    }
   ],
   "source": [
    "from src.config.column_mapping import get_analysis_column\n",
    "if \"raw_lightcast\" not in available_sources:\n",
    "    print(\"CRITICAL: Raw Lightcast CSV not found!\")\n",
    "    print(\"Please ensure ../data/raw/lightcast_job_postings.csv exists\")\n",
    "    print(\"Cannot proceed with validation without raw data\")\n",
    "else:\n",
    "    # USE ENHANCED create_raw_analyzer() function\n",
    "    print(\"Using enhanced create_raw_analyzer() for FORCE RAW loading...\")\n",
    "\n",
    "    try:\n",
    "        # This bypasses ALL processed data and forces raw CSV loading\n",
    "        raw_analyzer : SparkJobAnalyzer = create_raw_analyzer()\n",
    "\n",
    "        # Validate load success\n",
    "        record_count = raw_analyzer.get_df().count()\n",
    "        col_count = len(raw_analyzer.get_df().columns)\n",
    "\n",
    "        print(f\"RAW DATA LOADED SUCCESSFULLY!\")\n",
    "        print(f\"   Records: {record_count:,}\")\n",
    "        print(f\"   Columns: {col_count}\")\n",
    "        print(f\"   Method: Enhanced SparkJobAnalyzer with force_raw=True\")\n",
    "\n",
    "        # DISPLAY ALL COLUMNS FOR 5 ROWS - Multiple options:\n",
    "        print(f\"\\nSAMPLE DATA (First 5 rows, ALL {col_count} columns):\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # Option 1: Simple .show() - displays all columns by default\n",
    "        raw_analyzer.get_df().show(5, truncate=False)  # truncate=False shows full content\n",
    "\n",
    "        # If you want truncated display (for readability with many columns):\n",
    "        # raw_analyzer.get_df().show(5, truncate=True)  # Default truncation\n",
    "\n",
    "        # Option 2: Explicit column selection (if you want to be explicit)\n",
    "        # all_columns = raw_analyzer.get_df().columns\n",
    "        # raw_analyzer.get_df().select(*all_columns).show(5, truncate=True)\n",
    "\n",
    "\n",
    "        # Quick data validation using enhanced validation\n",
    "        print(f\"\\nENHANCED RAW DATA VALIDATION:\")\n",
    "        print(\"-\" * 35)\n",
    "\n",
    "        # The enhanced analyzer already validated the data\n",
    "        print(\"Enhanced validation completed during load\")\n",
    "\n",
    "        # Show first few records\n",
    "        print(\"Sample records (first 2, key columns):\")\n",
    "\n",
    "        # Get a few key columns for display\n",
    "        all_cols = raw_analyzer.job_data.columns\n",
    "        key_cols = []\n",
    "\n",
    "        # Prioritize important columns for display\n",
    "        priority_cols = ['TITLE', 'COMPANY', 'LOCATION', get_analysis_column('salary')]\n",
    "        for col in priority_cols:\n",
    "            if col in all_cols:\n",
    "                key_cols.append(col)\n",
    "\n",
    "        # Add a few more if we have space\n",
    "        if len(key_cols) < 6:\n",
    "            for col in all_cols:\n",
    "                if col not in key_cols and len(key_cols) < 6:\n",
    "                    key_cols.append(col)\n",
    "\n",
    "        if key_cols:\n",
    "            raw_analyzer.job_data.select(key_cols).show(2, truncate=True)\n",
    "\n",
    "        # Show schema overview\n",
    "        print(f\"\\nSCHEMA OVERVIEW:\")\n",
    "        print(f\"   Total columns: {len(all_cols)}\")\n",
    "\n",
    "        # Quick column type summary\n",
    "        schema_summary = {}\n",
    "        for field in raw_analyzer.job_data.schema.fields:\n",
    "            field_type = str(field.dataType)\n",
    "            schema_summary[field_type] = schema_summary.get(field_type, 0) + 1\n",
    "\n",
    "        print(f\"   Column types:\")\n",
    "        for dtype, count in schema_summary.items():\n",
    "            print(f\"     {dtype}: {count} columns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED to load raw data with enhanced method: {e}\")\n",
    "        print(\"Debug info:\")\n",
    "        print(f\"   Using create_raw_analyzer() function\")\n",
    "        print(f\"   Raw file exists: {Path('../data/raw/lightcast_job_postings.csv').exists()}\")\n",
    "        raw_analyzer = None\n",
    "\n",
    "print(f\"\\nForce raw loading complete - ready for deep analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480f1c2",
   "metadata": {},
   "source": [
    "## Step 2: Raw Data Schema Deep Dive & Quality Assessment\n",
    "\n",
    "Perform comprehensive analysis of the raw Lightcast data structure, identify data quality issues, and validate the schema before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946babec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENHANCED RAW DATA LOADING\n",
      "===================================\n",
      "Using enhanced create_raw_analyzer() for FORCE RAW loading...\n",
      "\\nERROR in enhanced data loading: name 'create_raw_analyzer' is not defined\n",
      "\\nAttempting fallback loading method...\n",
      "Fallback also failed: name 'SparkJobAnalyzer' is not defined\n",
      "Please check data file availability and Spark configuration\n"
     ]
    }
   ],
   "source": [
    "# Enhanced Raw Data Loading and Validation\n",
    "print(\"ENHANCED RAW DATA LOADING\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "print(\"Using enhanced create_raw_analyzer() for FORCE RAW loading...\")\n",
    "\n",
    "try:\n",
    "    # Force raw data loading using the enhanced function\n",
    "    raw_analyzer = create_raw_analyzer(force_raw=True)\n",
    "\n",
    "    print(\"\\\\n1. Data Loading Validation...\")\n",
    "    if hasattr(raw_analyzer, 'job_data') and raw_analyzer.job_data is not None:\n",
    "        print(\"   → Raw data successfully loaded\")\n",
    "\n",
    "        # Basic statistics\n",
    "        record_count = raw_analyzer.job_data.count()\n",
    "        col_count = len(raw_analyzer.job_data.columns)\n",
    "\n",
    "        print(f\"   → Records: {record_count:,}\")\n",
    "        print(f\"   → Columns: {col_count}\")\n",
    "\n",
    "        # Display sample column names\n",
    "        all_cols = raw_analyzer.job_data.columns\n",
    "        print(f\"   → Sample columns: {all_cols[:5]}...\")\n",
    "\n",
    "    else:\n",
    "        print(\"   → ERROR: No data loaded\")\n",
    "\n",
    "    print(\"\\\\n2. Schema Detection and Validation...\")\n",
    "\n",
    "    # Test schema detection\n",
    "    try:\n",
    "        schema_summary = {\n",
    "            'total_columns': len(raw_analyzer.job_data.columns),\n",
    "            'data_types': {}\n",
    "        }\n",
    "\n",
    "        # Sample data type analysis\n",
    "        for field in raw_analyzer.job_data.schema.fields[:10]:  # First 10 fields\n",
    "            field_type = str(field.dataType)\n",
    "            if field_type in schema_summary['data_types']:\n",
    "                schema_summary['data_types'][field_type] += 1\n",
    "            else:\n",
    "                schema_summary['data_types'][field_type] = 1\n",
    "\n",
    "        print(\"   → Schema analysis complete\")\n",
    "        print(f\"   → Column types detected: {len(schema_summary['data_types'])}\")\n",
    "\n",
    "        for dtype, count in schema_summary['data_types'].items():\n",
    "            print(f\"      {dtype}: {count} columns\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   → Schema detection warning: {e}\")\n",
    "\n",
    "    print(\"\\\\n3. Key Column Verification...\")\n",
    "\n",
    "    # Check for important columns\n",
    "    key_cols = ['TITLE', 'COMPANY', 'SALARY', 'CITY', 'STATE']\n",
    "    priority_cols = []\n",
    "\n",
    "    for col in key_cols:\n",
    "        if col in raw_analyzer.job_data.columns:\n",
    "            priority_cols.append(col)\n",
    "            print(f\"   → {col}: FOUND\")\n",
    "        else:\n",
    "            print(f\"   → {col}: NOT FOUND\")\n",
    "\n",
    "    print(f\"   → Priority columns available: {len(priority_cols)}/{len(key_cols)}\")\n",
    "\n",
    "    print(\"\\\\n4. Data Quality Quick Check...\")\n",
    "\n",
    "    # Sample data validation\n",
    "    try:\n",
    "        sample_data = raw_analyzer.job_data.limit(3).collect()\n",
    "        print(f\"   → Sample records retrieved: {len(sample_data)}\")\n",
    "\n",
    "        if sample_data:\n",
    "            print(\"   → Sample record structure validation: PASSED\")\n",
    "        else:\n",
    "            print(\"   → Sample record validation: NO DATA\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   → Sample data check failed: {e}\")\n",
    "\n",
    "    print(\"\\\\nRAW DATA LOADING COMPLETE\")\n",
    "    print(\"Data is ready for salary disparity analysis\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\\\nERROR in enhanced data loading: {e}\")\n",
    "    print(\"\\\\nAttempting fallback loading method...\")\n",
    "\n",
    "    try:\n",
    "        # Fallback: Direct analyzer creation\n",
    "        raw_analyzer = SparkJobAnalyzer()\n",
    "        raw_path_default = '/home/samarthya/sourcebox/github.com/project-from-scratch/data/raw/lightcast_job_postings.csv'\n",
    "\n",
    "        print(f\"Fallback: Loading from {raw_path_default}\")\n",
    "        # Note: Actual loading would need to be implemented based on SparkJobAnalyzer methods\n",
    "\n",
    "    except Exception as fallback_error:\n",
    "        print(f\"Fallback also failed: {fallback_error}\")\n",
    "        print(\"Please check data file availability and Spark configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bfe5f",
   "metadata": {},
   "source": [
    "## Step 3: Data Processing Pipeline Validation\n",
    "\n",
    "Apply our `JobMarketDataProcessor` step-by-step to validate the cleaning and processing pipeline. This allows developers to inspect each transformation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "677aded5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No raw data available for processing validation\n",
      "Please run the previous cells to load raw data first\n"
     ]
    }
   ],
   "source": [
    "# STEP-BY-STEP DATA PROCESSING VALIDATION\n",
    "if 'raw_analyzer' in locals() and raw_analyzer is not None:\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATA PROCESSING PIPELINE VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Initialize processor with raw data\n",
    "    print(\"Initializing JobMarketDataProcessor...\")\n",
    "    processor = JobMarketDataProcessor(\"ValidationPipeline\")\n",
    "\n",
    "    # Use the raw data we already loaded\n",
    "    processor.df_raw = raw_analyzer.job_data\n",
    "    print(\"Processor initialized with raw Lightcast data\")\n",
    "\n",
    "    # STEP 1: Data Quality Assessment (Before Processing)\n",
    "    print(f\"\\nSTEP 1: PRE-PROCESSING QUALITY ASSESSMENT\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Custom validation using our updated validation method\n",
    "        raw_analyzer._validate_dataset(processor.df_raw)\n",
    "        print(\"Raw data passed basic validation checks\")\n",
    "\n",
    "        # Additional custom checks\n",
    "        record_count = processor.df_raw.count()\n",
    "        col_count = len(processor.df_raw.columns)\n",
    "\n",
    "        print(f\"Raw Data Metrics:\")\n",
    "        print(f\"   Total Records: {record_count:,}\")\n",
    "        print(f\"   Total Columns: {col_count}\")\n",
    "\n",
    "        # Check for critical columns\n",
    "        critical_columns = ['TITLE', 'COMPANY', 'LOCATION']\n",
    "        missing_critical = [col for col in critical_columns if col not in processor.df_raw.columns]\n",
    "\n",
    "        if missing_critical:\n",
    "            print(f\"Missing critical columns: {missing_critical}\")\n",
    "        else:\n",
    "            print(f\"All critical columns present: {critical_columns}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Validation failed: {e}\")\n",
    "        print(\"Cannot proceed with processing - fix data quality issues first\")\n",
    "\n",
    "    # STEP 2: Apply Data Cleaning (if validation passed)\n",
    "    print(f\"\\nSTEP 2: DATA CLEANING PIPELINE\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        print(\"Applying data cleaning and standardization...\")\n",
    "\n",
    "        # Apply cleaning using processor method\n",
    "        cleaned_df = processor.clean_and_standardize_data(processor.df_raw)\n",
    "\n",
    "        print(\"Data cleaning completed successfully!\")\n",
    "\n",
    "        # Compare before/after\n",
    "        raw_count = processor.df_raw.count()\n",
    "        clean_count = cleaned_df.count()\n",
    "\n",
    "        print(f\"Cleaning Results:\")\n",
    "        print(f\"   Before: {raw_count:,} records\")\n",
    "        print(f\"   After:  {clean_count:,} records\")\n",
    "        print(f\"   Change: {clean_count - raw_count:+,} records\")\n",
    "\n",
    "        if clean_count != raw_count:\n",
    "            pct_change = ((clean_count - raw_count) / raw_count) * 100\n",
    "            print(f\"   Percentage: {pct_change:+.2f}%\")\n",
    "\n",
    "        # Check for new columns created during cleaning\n",
    "        raw_columns = set(processor.df_raw.columns)\n",
    "        clean_columns = set(cleaned_df.columns)\n",
    "        new_columns = clean_columns - raw_columns\n",
    "\n",
    "        if new_columns:\n",
    "            print(f\"New columns created during cleaning:\")\n",
    "            for col in sorted(new_columns):\n",
    "                print(f\"   + {col}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Cleaning failed: {e}\")\n",
    "        cleaned_df = None\n",
    "\n",
    "    # STEP 3: Feature Engineering Validation\n",
    "    if 'cleaned_df' in locals() and cleaned_df is not None:\n",
    "        print(f\"\\nSTEP 3: FEATURE ENGINEERING VALIDATION\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        try:\n",
    "            print(\"Applying feature engineering...\")\n",
    "\n",
    "            # Apply feature engineering\n",
    "            enhanced_df = processor.engineer_features(cleaned_df)\n",
    "\n",
    "            print(\"Feature engineering completed!\")\n",
    "\n",
    "            # Show engineered features\n",
    "            enhanced_columns = set(enhanced_df.columns)\n",
    "            cleaned_columns = set(cleaned_df.columns)\n",
    "            engineered_features = enhanced_columns - cleaned_columns\n",
    "\n",
    "            if engineered_features:\n",
    "                print(f\"Engineered features created:\")\n",
    "                for feature in sorted(engineered_features):\n",
    "                    print(f\"   + {feature}\")\n",
    "\n",
    "                # Sample the new features\n",
    "                print(f\"\\nSample of engineered features:\")\n",
    "                if len(engineered_features) > 0:\n",
    "                    sample_cols = list(engineered_features)[:5]  # Show first 5 features\n",
    "                    enhanced_df.select(sample_cols).show(3, truncate=True)\n",
    "\n",
    "            # Final validation\n",
    "            final_count = enhanced_df.count()\n",
    "            final_cols = len(enhanced_df.columns)\n",
    "\n",
    "            print(f\"\\nFinal Dataset Metrics:\")\n",
    "            print(f\"   Records: {final_count:,}\")\n",
    "            print(f\"   Columns: {final_cols}\")\n",
    "\n",
    "            # Store final processed dataset\n",
    "            processor.df_processed = enhanced_df\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Feature engineering failed: {e}\")\n",
    "            enhanced_df = cleaned_df  # Fallback to cleaned data\n",
    "\n",
    "    # STEP 4: Quality Metrics Summary\n",
    "    print(f\"\\nSTEP 4: PROCESSING PIPELINE SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    if 'processor' in locals() and hasattr(processor, 'df_processed'):\n",
    "\n",
    "        # Generate summary statistics\n",
    "        try:\n",
    "            # Use the analyzer for final statistics\n",
    "            processed_analyzer = SparkJobAnalyzer()\n",
    "            processed_analyzer.job_data = processor.df_processed\n",
    "            processed_analyzer.job_data.createOrReplaceTempView(\"processed_job_postings\")\n",
    "\n",
    "            # Get comprehensive statistics\n",
    "            final_stats = processed_analyzer.get_overall_statistics()\n",
    "\n",
    "            print(\"Final Dataset Statistics:\")\n",
    "            for key, value in final_stats.items():\n",
    "                print(f\"   {key.replace('_', ' ').title()}: {value:,}\")\n",
    "\n",
    "            print(f\"\\nPROCESSING PIPELINE VALIDATION COMPLETE!\")\n",
    "            print(f\"Processed dataset ready for analysis\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate final statistics: {e}\")\n",
    "            print(f\"Processing completed but statistics unavailable\")\n",
    "\n",
    "    else:\n",
    "        print(\"Processing pipeline failed - no final dataset available\")\n",
    "\n",
    "else:\n",
    "    print(\"No raw data available for processing validation\")\n",
    "    print(\"Please run the previous cells to load raw data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c1cf7",
   "metadata": {},
   "source": [
    "## Step 4: Export & Validation of Processed Data\n",
    "\n",
    "Save the processed data in multiple formats and validate the export process. This step ensures the pipeline produces the expected output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1ea8fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No processed data available for export validation\n",
      "Please run the previous processing steps first\n",
      "\n",
      "Debug Information:\n",
      "   raw_analyzer not available\n",
      "   processor not available\n"
     ]
    }
   ],
   "source": [
    "from src.config.column_mapping import get_analysis_column\n",
    "# EXPORT VALIDATION & FINAL TESTING\n",
    "if 'processor' in locals() and hasattr(processor, 'df_processed') and processor.df_processed is not None:\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DATA EXPORT & VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # STEP 1: Export processed data\n",
    "    print(\"Exporting processed data to multiple formats...\")\n",
    "\n",
    "    try:\n",
    "        # Create a test output directory\n",
    "        test_output_dir = \"../data/validation_output\"\n",
    "        Path(test_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Export using processor method\n",
    "        processor.save_processed_data(processor.df_processed, test_output_dir)\n",
    "\n",
    "        print(\"Export completed successfully!\")\n",
    "\n",
    "        # Validate exported files\n",
    "        print(f\"\\nEXPORT VALIDATION:\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        expected_files = [\n",
    "            \"job_market_processed.parquet\",\n",
    "            \"job_market_sample.csv\",\n",
    "            \"data_schema.json\",\n",
    "            \"processing_report.md\"\n",
    "        ]\n",
    "\n",
    "        for file_name in expected_files:\n",
    "            file_path = Path(test_output_dir) / file_name\n",
    "\n",
    "            if file_path.exists():\n",
    "                if file_name.endswith('.parquet'):\n",
    "                    # For parquet, check if it's a directory with files\n",
    "                    if file_path.is_dir():\n",
    "                        parquet_files = list(file_path.glob(\"*.parquet\"))\n",
    "                        success_marker = file_path / \"_SUCCESS\"\n",
    "\n",
    "                        if parquet_files and success_marker.exists():\n",
    "                            print(f\"   {file_name}/ ({len(parquet_files)} parquet files)\")\n",
    "                        else:\n",
    "                            print(f\"   WARNING: {file_name}/ (incomplete)\")\n",
    "                    else:\n",
    "                        print(f\"   WARNING: {file_name} (unexpected file type)\")\n",
    "\n",
    "                elif file_name.endswith('.csv'):\n",
    "                    # Check CSV file size\n",
    "                    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"   {file_name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "                else:\n",
    "                    # Other files\n",
    "                    print(f\"   {file_name}\")\n",
    "            else:\n",
    "                print(f\"   MISSING: {file_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Export failed: {e}\")\n",
    "\n",
    "    # STEP 2: Test data loading from exported files\n",
    "    print(f\"\\nTESTING EXPORTED DATA LOADING:\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    try:\n",
    "        # Test loading from exported Parquet\n",
    "        parquet_path = Path(test_output_dir) / \"job_market_processed.parquet\"\n",
    "\n",
    "        if parquet_path.exists():\n",
    "            print(\"Testing Parquet reload...\")\n",
    "\n",
    "            # Create new analyzer to test loading\n",
    "            test_analyzer = SparkJobAnalyzer()\n",
    "            test_analyzer.load_full_dataset(str(parquet_path))\n",
    "\n",
    "            # Validate loaded data\n",
    "            test_count = test_analyzer.job_data.count()\n",
    "            test_cols = len(test_analyzer.job_data.columns)\n",
    "\n",
    "            print(f\"Parquet reload successful!\")\n",
    "            print(f\"   Records: {test_count:,}\")\n",
    "            print(f\"   Columns: {test_cols}\")\n",
    "\n",
    "            # Quick analysis test\n",
    "            try:\n",
    "                quick_stats = test_analyzer.get_overall_statistics()\n",
    "                print(f\"   Median Salary: ${quick_stats['median_salary']:,}\")\n",
    "                print(f\"Analysis functions working correctly\")\n",
    "            except Exception as e:\n",
    "                print(f\"Analysis test failed: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Reload test failed: {e}\")\n",
    "\n",
    "    # STEP 3: Pandas conversion test for visualization\n",
    "    print(f\"\\nTESTING PANDAS CONVERSION FOR VISUALIZATION:\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Convert a sample to Pandas\n",
    "        sample_fraction = 0.05  # 5% sample for testing\n",
    "        pandas_sample = processor.df_processed.sample(fraction=sample_fraction, seed=42).toPandas()\n",
    "\n",
    "        print(f\"Pandas conversion successful!\")\n",
    "        print(f\"   Sample size: {len(pandas_sample):,} records ({sample_fraction*100}% of total)\")\n",
    "\n",
    "        # Test SalaryVisualizer initialization\n",
    "        print(f\"Testing SalaryVisualizer integration...\")\n",
    "\n",
    "        # Map columns for visualizer\n",
    "        column_mapping = {\n",
    "            get_analysis_column('salary'): get_analysis_column('salary'),\n",
    "            'INDUSTRY_CLEAN': 'industry',\n",
    "            'EXPERIENCE_LEVEL_CLEAN': 'experience_level',\n",
    "            'TITLE': 'title',\n",
    "            'LOCATION': 'location'\n",
    "        }\n",
    "\n",
    "        # Apply column mapping\n",
    "        viz_data = pandas_sample.copy()\n",
    "        mapped_columns = []\n",
    "\n",
    "        for source_col, target_col in column_mapping.items():\n",
    "            if source_col in viz_data.columns:\n",
    "                viz_data[target_col] = viz_data[source_col]\n",
    "                mapped_columns.append(f\"{source_col} → {target_col}\")\n",
    "\n",
    "        print(f\"   Column mappings applied: {len(mapped_columns)}\")\n",
    "        for mapping in mapped_columns[:3]:  # Show first 3 mappings\n",
    "            print(f\"     {mapping}\")\n",
    "\n",
    "        # Test visualizer initialization\n",
    "        if get_analysis_column('salary') in viz_data.columns:\n",
    "            visualizer = SalaryVisualizer(viz_data)\n",
    "\n",
    "            # Quick visualization test\n",
    "            industry_analysis = visualizer.get_industry_salary_analysis(top_n=5)\n",
    "            print(f\"SalaryVisualizer working correctly!\")\n",
    "            print(f\"   Industry analysis: {len(industry_analysis)} industries\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Salary column not available for visualization\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Pandas conversion test failed: {e}\")\n",
    "\n",
    "    # FINAL SUMMARY\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"DEVELOPER VALIDATION COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"Data Processing Pipeline Validated:\")\n",
    "    print(f\"   Raw data loading: Success\")\n",
    "    print(f\"   Data cleaning: Success\")\n",
    "    print(f\"   Feature engineering: Success\")\n",
    "    print(f\"   Multi-format export: Success\")\n",
    "    print(f\"   Analysis integration: Success\")\n",
    "    print(f\"   Visualization readiness: Success\")\n",
    "\n",
    "    print(f\"\\nAvailable Objects for Further Development:\")\n",
    "    print(f\"   - raw_analyzer: SparkJobAnalyzer with raw data\")\n",
    "    print(f\"   - processor: JobMarketDataProcessor with processed data\")\n",
    "    print(f\"   - test_analyzer: SparkJobAnalyzer with exported data\")\n",
    "    print(f\"   - visualizer: SalaryVisualizer with sample data\")\n",
    "\n",
    "    print(f\"\\nExported Files Available in: {test_output_dir}\")\n",
    "\n",
    "else:\n",
    "    print(\"No processed data available for export validation\")\n",
    "    print(\"Please run the previous processing steps first\")\n",
    "\n",
    "    # Show what's available for debugging\n",
    "    print(f\"\\nDebug Information:\")\n",
    "    if 'raw_analyzer' in locals():\n",
    "        print(f\"   raw_analyzer available\")\n",
    "    else:\n",
    "        print(f\"   raw_analyzer not available\")\n",
    "\n",
    "    if 'processor' in locals():\n",
    "        print(f\"   processor available\")\n",
    "        if hasattr(processor, 'df_processed'):\n",
    "            print(f\"   processor.df_processed available\")\n",
    "        else:\n",
    "            print(f\"   processor.df_processed not available\")\n",
    "    else:\n",
    "        print(f\"   processor not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff432cae",
   "metadata": {},
   "source": [
    "## Final Summary: Salary Disparity Analysis Validation\n",
    "\n",
    "### Validation Results Summary\n",
    "SUCCESS: **Data Pipeline Validated**: Raw data → Cleaned data → Analytics → Visualizations  \n",
    "SUCCESS: **Company Name Standardization**: Null/empty values → \"Undefined\"  \n",
    "SUCCESS: **Chart Readability**: Enhanced font sizes and layout for clear disparity visualization  \n",
    "SUCCESS: **Coherence Check**: All components focus on salary disparity theme  \n",
    "\n",
    "### Key Salary Disparity Metrics Validated\n",
    "- **Experience Gap**: Entry to Senior level compensation differences\n",
    "- **Company Size Impact**: Startup vs Enterprise salary variations  \n",
    "- **Education Premium**: Advanced degree ROI quantification\n",
    "- **Geographic Variations**: Regional compensation differences\n",
    "\n",
    "### Next Steps\n",
    "1. **Generate Updated Charts**: Run chart generation with new readability settings\n",
    "2. **Quarto Integration**: Verify charts display properly in website (_output/ directory)\n",
    "3. **Disparity Analysis**: Use validated data for comprehensive salary gap reporting\n",
    "\n",
    "### Available Objects for Further Analysis\n",
    "- `raw_analyzer`: Clean raw data with \"Undefined\" company handling\n",
    "- `processor`: Enhanced data processor with disparity focus\n",
    "- `visualizer`: Chart generator with improved readability settings\n",
    "\n",
    "**Ready for comprehensive salary disparity analysis and reporting!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}