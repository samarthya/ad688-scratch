{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b54e1a6",
   "metadata": {},
   "source": [
    "# Job Market Data Processing: Developer Validation Interface\n",
    "\n",
    "This notebook serves as a **developer interface** for validating each step of the job market data processing pipeline. It forces fresh data loads, validates processing steps, and provides detailed diagnostics for debugging and quality assurance.\n",
    "\n",
    "## üéØ Purpose\n",
    "- **Force reload** data from raw sources (no cached/processed data)\n",
    "- **Step-by-step validation** of data processing pipeline\n",
    "- **Quality assessment** at each processing stage\n",
    "- **Developer debugging** interface for troubleshooting data issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aef793",
   "metadata": {},
   "source": [
    "## Step 1: Initialize Clean Environment & Force Raw Data Loading\n",
    "\n",
    "We'll start fresh by clearing any cached data and forcing the system to load from the original raw Lightcast CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f436fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Reloading spark_analyzer module to pick up path fixes...\n",
      "‚úÖ Enhanced classes imported successfully:\n",
      "   - SparkJobAnalyzer: Raw data loading & SQL analysis\n",
      "   - create_raw_analyzer: Force raw data loading function\n",
      "   - JobMarketDataProcessor: Data cleaning & processing pipeline\n",
      "   - SalaryVisualizer: Visualization utilities\n",
      "‚úÖ create_raw_analyzer function: <function create_raw_analyzer at 0x7c71dab153a0>\n",
      "\n",
      "üîß Function source verification: create_raw_analyzer calls create_spark_analyzer(force_raw=True)\n",
      "üîß Default raw data path in module: ../../data/raw/lightcast_job_postings.csv\n",
      "\n",
      "üßπ Clearing existing Spark session for clean validation...\n",
      "   ‚úÖ Active Spark session stopped\n",
      "   ‚úÖ No local spark variables found\n",
      "\n",
      "üéØ Ready for force raw data loading and validation!\n",
      "   ‚úÖ Active Spark session stopped\n",
      "   ‚úÖ No local spark variables found\n",
      "\n",
      "üéØ Ready for force raw data loading and validation!\n"
     ]
    }
   ],
   "source": [
    "# Clean environment setup for developer validation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import importlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# FORCE RELOAD MODULE TO PICK UP PATH FIXES\n",
    "if 'data.spark_analyzer' in sys.modules:\n",
    "    print(\"üîÑ Reloading spark_analyzer module to pick up path fixes...\")\n",
    "    importlib.reload(sys.modules['data.spark_analyzer'])\n",
    "else:\n",
    "    print(\"üîÑ Loading spark_analyzer module for the first time...\")\n",
    "\n",
    "from data.spark_analyzer import SparkJobAnalyzer, create_raw_analyzer\n",
    "from data.enhanced_processor import JobMarketDataProcessor  \n",
    "from visualization.simple_plots import SalaryVisualizer\n",
    "    \n",
    "print(\"‚úÖ Enhanced classes imported successfully:\")\n",
    "print(\"   - SparkJobAnalyzer: Raw data loading & SQL analysis\") \n",
    "print(\"   - create_raw_analyzer: Force raw data loading function\")\n",
    "print(\"   - JobMarketDataProcessor: Data cleaning & processing pipeline\")\n",
    "print(\"   - SalaryVisualizer: Visualization utilities\")\n",
    "\n",
    "# Verify the function is callable\n",
    "print(f\"‚úÖ create_raw_analyzer function: {create_raw_analyzer}\")\n",
    "\n",
    "# DEBUG: Check what path the function will actually use\n",
    "try:\n",
    "    import inspect\n",
    "    analyzer_code = inspect.getsource(create_raw_analyzer)\n",
    "    print(f\"\\nüîß Function source verification: create_raw_analyzer calls create_spark_analyzer(force_raw=True)\")\n",
    "    \n",
    "    # Check the actual default path in the module\n",
    "    import data.spark_analyzer as sa\n",
    "    load_method = getattr(sa.SparkJobAnalyzer, '_load_raw_data')\n",
    "    signature = inspect.signature(load_method)\n",
    "    raw_path_default = signature.parameters['raw_data_path'].default\n",
    "    print(f\"üîß Default raw data path in module: {raw_path_default}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not inspect function details: {e}\")\n",
    "  \n",
    "clear_spark = True  # Set to False to skip clearing\n",
    "\n",
    "if clear_spark:\n",
    "    try:\n",
    "        from pyspark.sql import SparkSession\n",
    "        \n",
    "        # Use proper public API to check for existing sessions\n",
    "        active_session = SparkSession.getActiveSession()\n",
    "        \n",
    "        if active_session is not None:\n",
    "            print(\"\\nüßπ Clearing existing Spark session for clean validation...\")\n",
    "            active_session.stop()\n",
    "            print(\"   ‚úÖ Active Spark session stopped\")\n",
    "        else:\n",
    "            print(\"\\n‚úÖ No active Spark session found - environment is clean\")\n",
    "            \n",
    "        # Clean up any local spark variables (defensive programming)\n",
    "        # This removes spark variables that might exist in notebook memory\n",
    "        spark_vars = [var for var in locals() if var.startswith('spark')]\n",
    "        \n",
    "        if spark_vars:\n",
    "            print(f\"   üßπ Clearing local spark variables: {spark_vars}\")\n",
    "            for var in spark_vars:\n",
    "                del locals()[var]\n",
    "            print(\"   ‚úÖ Local spark variables cleared\")\n",
    "        else:\n",
    "            print(\"   ‚úÖ No local spark variables found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è  Note: Could not clear Spark sessions: {e}\")\n",
    "        print(\"   This is usually fine - continuing with validation...\")\n",
    "\n",
    "print(f\"\\nüéØ Ready for force raw data loading and validation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0e2f785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üöÄ FORCING RAW DATA LOAD FOR VALIDATION\n",
      "============================================================\n",
      "üìÇ DATA SOURCE AVAILABILITY CHECK:\n",
      "----------------------------------------\n",
      "   raw_lightcast       : ‚úÖ EXISTS\n",
      "                         üìä Size: 683.5 MB\n",
      "   processed_parquet   : ‚ùå MISSING\n",
      "   clean_csv           : ‚ùå MISSING\n",
      "\n",
      "üéØ DEVELOPER MODE: FORCING RAW DATA LOAD\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# FORCE LOAD RAW DATA - Developer Validation Mode\n",
    "print(\"=\" * 60)\n",
    "print(\"üöÄ FORCING RAW DATA LOAD FOR VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define data paths to check\n",
    "data_sources = {\n",
    "    \"raw_lightcast\": \"../data/raw/lightcast_job_postings.csv\",\n",
    "    \"processed_parquet\": \"../data/processed/job_market_processed.parquet\",\n",
    "    \"clean_csv\": \"../data/processed/clean_job_data.csv\"\n",
    "}\n",
    "\n",
    "# Check what data sources exist\n",
    "print(\"üìÇ DATA SOURCE AVAILABILITY CHECK:\")\n",
    "print(\"-\" * 40)\n",
    "available_sources = {}\n",
    "for source_name, path in data_sources.items():\n",
    "    exists = Path(path).exists()\n",
    "    status = \"‚úÖ EXISTS\" if exists else \"‚ùå MISSING\"\n",
    "    print(f\"   {source_name:<20}: {status}\")\n",
    "    if exists:\n",
    "        if path.endswith('.csv'):\n",
    "            # For CSV files, check file size\n",
    "            size_mb = Path(path).stat().st_size / (1024 * 1024)\n",
    "            print(f\"   {'':<20}  üìä Size: {size_mb:.1f} MB\")\n",
    "        available_sources[source_name] = path\n",
    "\n",
    "print(f\"\\nüéØ DEVELOPER MODE: FORCING RAW DATA LOAD\")\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bb970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n",
      "INFO:py4j.clientserver:Closing down clientserver connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Using enhanced create_raw_analyzer() for FORCE RAW loading...\n",
      "‚ùå FAILED to load raw data with enhanced method: [Errno 111] Connection refused\n",
      "üîß Debug info:\n",
      "   Using create_raw_analyzer() function\n",
      "   Raw file exists: True\n",
      "\n",
      "‚úÖ Force raw loading complete - ready for deep analysis!\n"
     ]
    }
   ],
   "source": [
    "if \"raw_lightcast\" not in available_sources:\n",
    "    print(\"‚ùå CRITICAL: Raw Lightcast CSV not found!\")\n",
    "    print(\"üí° Please ensure ../data/raw/lightcast_job_postings.csv exists\")\n",
    "    print(\"üõë Cannot proceed with validation without raw data\")\n",
    "else:\n",
    "    # USE ENHANCED create_raw_analyzer() function\n",
    "    print(\"üîÑ Using enhanced create_raw_analyzer() for FORCE RAW loading...\")\n",
    "    \n",
    "    try:\n",
    "        # This bypasses ALL processed data and forces raw CSV loading\n",
    "        raw_analyzer : SparkJobAnalyzer = create_raw_analyzer()\n",
    "        \n",
    "        # Validate load success\n",
    "        record_count = raw_analyzer.get_df().count()\n",
    "        col_count = len(raw_analyzer.get_df().columns)\n",
    "        \n",
    "        print(f\"‚úÖ RAW DATA LOADED SUCCESSFULLY!\")\n",
    "        print(f\"   üìä Records: {record_count:,}\")\n",
    "        print(f\"   üìã Columns: {col_count}\")\n",
    "        print(f\"   üîß Method: Enhanced SparkJobAnalyzer with force_raw=True\")\n",
    "        \n",
    "        # raw_analyzer.get_df().toPandas().head(5)\n",
    "        \n",
    "        \n",
    "\n",
    "        print(f\"--- Saurabh\")\n",
    "        # Quick data validation using enhanced validation\n",
    "        print(f\"\\nüîç ENHANCED RAW DATA VALIDATION:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        # The enhanced analyzer already validated the data\n",
    "        print(\"‚úÖ Enhanced validation completed during load\")\n",
    "        \n",
    "        # Show first few records\n",
    "        print(\"üìù Sample records (first 2, key columns):\")\n",
    "        \n",
    "        # Get a few key columns for display\n",
    "        all_cols = raw_analyzer.job_data.columns\n",
    "        key_cols = []\n",
    "        \n",
    "        # Prioritize important columns for display\n",
    "        priority_cols = ['TITLE', 'COMPANY', 'LOCATION', 'SALARY_AVG_IMPUTED']\n",
    "        for col in priority_cols:\n",
    "            if col in all_cols:\n",
    "                key_cols.append(col)\n",
    "        \n",
    "        # Add a few more if we have space\n",
    "        if len(key_cols) < 6:\n",
    "            for col in all_cols:\n",
    "                if col not in key_cols and len(key_cols) < 6:\n",
    "                    key_cols.append(col)\n",
    "        \n",
    "        if key_cols:\n",
    "            raw_analyzer.job_data.select(key_cols).show(2, truncate=True)\n",
    "        \n",
    "        # Show schema overview\n",
    "        print(f\"\\nüîß SCHEMA OVERVIEW:\")\n",
    "        print(f\"   Total columns: {len(all_cols)}\")\n",
    "        \n",
    "        # Quick column type summary\n",
    "        schema_summary = {}\n",
    "        for field in raw_analyzer.job_data.schema.fields:\n",
    "            field_type = str(field.dataType)\n",
    "            schema_summary[field_type] = schema_summary.get(field_type, 0) + 1\n",
    "        \n",
    "        print(f\"   Column types:\")\n",
    "        for dtype, count in schema_summary.items():\n",
    "            print(f\"     {dtype}: {count} columns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå FAILED to load raw data with enhanced method: {e}\")\n",
    "        print(\"üîß Debug info:\")\n",
    "        print(f\"   Using create_raw_analyzer() function\")\n",
    "        print(f\"   Raw file exists: {Path('../data/raw/lightcast_job_postings.csv').exists()}\")\n",
    "        raw_analyzer = None\n",
    "        \n",
    "print(f\"\\n‚úÖ Force raw loading complete - ready for deep analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5480f1c2",
   "metadata": {},
   "source": [
    "## Step 2: Raw Data Schema Deep Dive & Quality Assessment\n",
    "\n",
    "Perform comprehensive analysis of the raw Lightcast data structure, identify data quality issues, and validate the schema before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946babec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE RAW DATA ANALYSIS - Developer Deep Dive\n",
    "if 'raw_analyzer' in locals() and raw_analyzer is not None and raw_analyzer.job_data is not None:\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"\udd2c RAW LIGHTCAST DATA DEEP DIVE ANALYSIS\")  \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    raw_df = raw_analyzer.job_data\n",
    "    \n",
    "    # 1. COMPLETE SCHEMA ANALYSIS\n",
    "    print(\"\udccb COMPLETE SCHEMA BREAKDOWN:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Dataset Shape: {raw_df.count():,} rows √ó {len(raw_df.columns)} columns\")\n",
    "    print(f\"\\nFull Schema:\")\n",
    "    raw_df.printSchema()\n",
    "    \n",
    "    # 2. COLUMN CATEGORIZATION FOR DEVELOPERS\n",
    "    all_columns = raw_df.columns\n",
    "    print(f\"\\nüè∑Ô∏è  COLUMN CATEGORIZATION ({len(all_columns)} total):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Categorize columns by purpose\n",
    "    column_categories = {\n",
    "        'IDENTITY': [col for col in all_columns if any(x in col.upper() for x in ['JOB_ID', 'ID'])],\n",
    "        'BASIC_INFO': [col for col in all_columns if any(x in col.upper() for x in ['TITLE', 'COMPANY', 'DESCRIPTION'])],\n",
    "        'LOCATION': [col for col in all_columns if any(x in col.upper() for x in ['LOCATION', 'CITY', 'STATE', 'COUNTRY'])],\n",
    "        'SALARY': [col for col in all_columns if 'SALARY' in col.upper()],\n",
    "        'EMPLOYMENT': [col for col in all_columns if any(x in col.upper() for x in ['EMPLOYMENT', 'EXPERIENCE', 'EDUCATION'])],\n",
    "        'REMOTE_AI': [col for col in all_columns if any(x in col.upper() for x in ['REMOTE', 'AI', 'IS_AI'])],\n",
    "        'INDUSTRY': [col for col in all_columns if 'INDUSTRY' in col.upper()],\n",
    "        'TEMPORAL': [col for col in all_columns if any(x in col.upper() for x in ['DATE', 'POSTED', 'TIME'])],\n",
    "        'OTHER': []\n",
    "    }\n",
    "    \n",
    "    # Assign uncategorized columns to OTHER\n",
    "    categorized = set()\n",
    "    for cat_cols in column_categories.values():\n",
    "        categorized.update(cat_cols)\n",
    "    column_categories['OTHER'] = [col for col in all_columns if col not in categorized]\n",
    "    \n",
    "    for category, cols in column_categories.items():\n",
    "        if cols:\n",
    "            print(f\"\\n{category} ({len(cols)} columns):\")\n",
    "            for col in cols:\n",
    "                print(f\"   ‚úÖ {col}\")\n",
    "    \n",
    "    # 3. DATA QUALITY DEEP DIVE\n",
    "    print(f\"\\nüîç DATA QUALITY ASSESSMENT:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Null value analysis\n",
    "    print(\"üìä NULL VALUE ANALYSIS:\")\n",
    "    null_analysis = []\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    batch_size = 10\n",
    "    for i in range(0, len(all_columns), batch_size):\n",
    "        batch_cols = all_columns[i:i+batch_size]\n",
    "        \n",
    "        for col in batch_cols:\n",
    "            try:\n",
    "                null_count = raw_df.filter(raw_df[col].isNull()).count()\n",
    "                total_count = raw_df.count()\n",
    "                null_pct = (null_count / total_count) * 100 if total_count > 0 else 0\n",
    "                \n",
    "                null_analysis.append({\n",
    "                    'column': col,\n",
    "                    'null_count': null_count,\n",
    "                    'null_percentage': null_pct\n",
    "                })\n",
    "                \n",
    "                if null_pct > 50:  # High null percentage\n",
    "                    print(f\"   ‚ö†Ô∏è  {col}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "                elif null_pct > 10:  # Moderate null percentage\n",
    "                    print(f\"   üî∂ {col}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "                elif null_count > 0:  # Some nulls\n",
    "                    print(f\"   ‚úÖ {col}: {null_count:,} nulls ({null_pct:.1f}%)\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ùå {col}: Error analyzing nulls - {e}\")\n",
    "    \n",
    "    # 4. SALARY DATA VALIDATION (CRITICAL FOR ANALYSIS)\n",
    "    salary_columns = column_categories['SALARY']\n",
    "    if salary_columns:\n",
    "        print(f\"\\nüí∞ SALARY DATA VALIDATION:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        for sal_col in salary_columns:\n",
    "            try:\n",
    "                # Basic stats for salary columns\n",
    "                non_null_count = raw_df.filter(raw_df[sal_col].isNotNull()).count()\n",
    "                \n",
    "                if non_null_count > 0:\n",
    "                    # Get basic statistics\n",
    "                    sal_stats = raw_df.select(sal_col).describe().collect()\n",
    "                    \n",
    "                    print(f\"\\n{sal_col}:\")\n",
    "                    print(f\"   Non-null records: {non_null_count:,}\")\n",
    "                    \n",
    "                    for stat in sal_stats:\n",
    "                        if stat['summary'] in ['min', 'max', 'mean']:\n",
    "                            try:\n",
    "                                value = float(stat[sal_col]) if stat[sal_col] else 0\n",
    "                                print(f\"   {stat['summary'].capitalize()}: ${value:,.0f}\")\n",
    "                            except:\n",
    "                                print(f\"   {stat['summary'].capitalize()}: {stat[sal_col]}\")\n",
    "                else:\n",
    "                    print(f\"\\n{sal_col}: All values are null\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"\\n{sal_col}: Error analyzing - {e}\")\n",
    "    \n",
    "    # 5. SAMPLE DATA INSPECTION\n",
    "    print(f\"\\n\udcdd SAMPLE RAW DATA (First 3 records):\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Show sample with key columns only (to avoid overwhelming output)\n",
    "    key_columns = []\n",
    "    for category in ['IDENTITY', 'BASIC_INFO', 'SALARY', 'LOCATION']:\n",
    "        key_columns.extend(column_categories.get(category, [])[:2])  # Max 2 cols per category\n",
    "    \n",
    "    if key_columns:\n",
    "        print(f\"Key columns shown: {key_columns}\")\n",
    "        raw_df.select(key_columns).show(3, truncate=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ RAW DATA ANALYSIS COMPLETE\")\n",
    "    print(f\"üìä Summary: {raw_df.count():,} records, {len(all_columns)} columns analyzed\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No raw data available for analysis\")\n",
    "    print(\"üí° Please run the previous cell to load raw data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78bfe5f",
   "metadata": {},
   "source": [
    "## Step 3: Data Processing Pipeline Validation\n",
    "\n",
    "Apply our `JobMarketDataProcessor` step-by-step to validate the cleaning and processing pipeline. This allows developers to inspect each transformation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677aded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP-BY-STEP DATA PROCESSING VALIDATION\n",
    "if 'raw_analyzer' in locals() and raw_analyzer is not None:\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üîß DATA PROCESSING PIPELINE VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize processor with raw data\n",
    "    print(\"üöÄ Initializing JobMarketDataProcessor...\")\n",
    "    processor = JobMarketDataProcessor(\"ValidationPipeline\")\n",
    "    \n",
    "    # Use the raw data we already loaded\n",
    "    processor.df_raw = raw_analyzer.job_data\n",
    "    print(\"‚úÖ Processor initialized with raw Lightcast data\")\n",
    "    \n",
    "    # STEP 1: Data Quality Assessment (Before Processing)\n",
    "    print(f\"\\n\udcca STEP 1: PRE-PROCESSING QUALITY ASSESSMENT\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Custom validation using our updated validation method\n",
    "        raw_analyzer._validate_dataset(processor.df_raw)\n",
    "        print(\"‚úÖ Raw data passed basic validation checks\")\n",
    "        \n",
    "        # Additional custom checks\n",
    "        record_count = processor.df_raw.count()\n",
    "        col_count = len(processor.df_raw.columns)\n",
    "        \n",
    "        print(f\"\udcc8 Raw Data Metrics:\")\n",
    "        print(f\"   Total Records: {record_count:,}\")\n",
    "        print(f\"   Total Columns: {col_count}\")\n",
    "        \n",
    "        # Check for critical columns\n",
    "        critical_columns = ['TITLE', 'COMPANY', 'LOCATION']\n",
    "        missing_critical = [col for col in critical_columns if col not in processor.df_raw.columns]\n",
    "        \n",
    "        if missing_critical:\n",
    "            print(f\"‚ö†Ô∏è  Missing critical columns: {missing_critical}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ All critical columns present: {critical_columns}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Validation failed: {e}\")\n",
    "        print(\"üõë Cannot proceed with processing - fix data quality issues first\")\n",
    "    \n",
    "    # STEP 2: Apply Data Cleaning (if validation passed)\n",
    "    print(f\"\\nüßπ STEP 2: DATA CLEANING PIPELINE\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        print(\"üîÑ Applying data cleaning and standardization...\")\n",
    "        \n",
    "        # Apply cleaning using processor method\n",
    "        cleaned_df = processor.clean_and_standardize_data(processor.df_raw)\n",
    "        \n",
    "        print(\"‚úÖ Data cleaning completed successfully!\")\n",
    "        \n",
    "        # Compare before/after\n",
    "        raw_count = processor.df_raw.count()\n",
    "        clean_count = cleaned_df.count()\n",
    "        \n",
    "        print(f\"üìä Cleaning Results:\")\n",
    "        print(f\"   Before: {raw_count:,} records\")\n",
    "        print(f\"   After:  {clean_count:,} records\")\n",
    "        print(f\"   Change: {clean_count - raw_count:+,} records\")\n",
    "        \n",
    "        if clean_count != raw_count:\n",
    "            pct_change = ((clean_count - raw_count) / raw_count) * 100\n",
    "            print(f\"   Percentage: {pct_change:+.2f}%\")\n",
    "            \n",
    "        # Check for new columns created during cleaning\n",
    "        raw_columns = set(processor.df_raw.columns)\n",
    "        clean_columns = set(cleaned_df.columns)\n",
    "        new_columns = clean_columns - raw_columns\n",
    "        \n",
    "        if new_columns:\n",
    "            print(f\"‚ú® New columns created during cleaning:\")\n",
    "            for col in sorted(new_columns):\n",
    "                print(f\"   + {col}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Cleaning failed: {e}\")\n",
    "        cleaned_df = None\n",
    "    \n",
    "    # STEP 3: Feature Engineering Validation\n",
    "    if 'cleaned_df' in locals() and cleaned_df is not None:\n",
    "        print(f\"\\n‚öôÔ∏è  STEP 3: FEATURE ENGINEERING VALIDATION\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        try:\n",
    "            print(\"üîÑ Applying feature engineering...\")\n",
    "            \n",
    "            # Apply feature engineering\n",
    "            enhanced_df = processor.engineer_features(cleaned_df)\n",
    "            \n",
    "            print(\"‚úÖ Feature engineering completed!\")\n",
    "            \n",
    "            # Show engineered features\n",
    "            enhanced_columns = set(enhanced_df.columns)\n",
    "            cleaned_columns = set(cleaned_df.columns)\n",
    "            engineered_features = enhanced_columns - cleaned_columns\n",
    "            \n",
    "            if engineered_features:\n",
    "                print(f\"üéØ Engineered features created:\")\n",
    "                for feature in sorted(engineered_features):\n",
    "                    print(f\"   + {feature}\")\n",
    "                    \n",
    "                # Sample the new features\n",
    "                print(f\"\\nüìä Sample of engineered features:\")\n",
    "                if len(engineered_features) > 0:\n",
    "                    sample_cols = list(engineered_features)[:5]  # Show first 5 features\n",
    "                    enhanced_df.select(sample_cols).show(3, truncate=True)\n",
    "            \n",
    "            # Final validation\n",
    "            final_count = enhanced_df.count()\n",
    "            final_cols = len(enhanced_df.columns)\n",
    "            \n",
    "            print(f\"\\nüìà Final Dataset Metrics:\")\n",
    "            print(f\"   Records: {final_count:,}\")\n",
    "            print(f\"   Columns: {final_cols}\")\n",
    "            \n",
    "            # Store final processed dataset\n",
    "            processor.df_processed = enhanced_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Feature engineering failed: {e}\")\n",
    "            enhanced_df = cleaned_df  # Fallback to cleaned data\n",
    "    \n",
    "    # STEP 4: Quality Metrics Summary\n",
    "    print(f\"\\nüìä STEP 4: PROCESSING PIPELINE SUMMARY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if 'processor' in locals() and hasattr(processor, 'df_processed'):\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        try:\n",
    "            # Use the analyzer for final statistics\n",
    "            processed_analyzer = SparkJobAnalyzer()\n",
    "            processed_analyzer.job_data = processor.df_processed\n",
    "            processed_analyzer.job_data.createOrReplaceTempView(\"processed_job_postings\")\n",
    "            \n",
    "            # Get comprehensive statistics\n",
    "            final_stats = processed_analyzer.get_overall_statistics()\n",
    "            \n",
    "            print(\"üìà Final Dataset Statistics:\")\n",
    "            for key, value in final_stats.items():\n",
    "                print(f\"   {key.replace('_', ' ').title()}: {value:,}\")\n",
    "                \n",
    "            print(f\"\\n‚úÖ PROCESSING PIPELINE VALIDATION COMPLETE!\")\n",
    "            print(f\"üéØ Processed dataset ready for analysis\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not generate final statistics: {e}\")\n",
    "            print(f\"‚úÖ Processing completed but statistics unavailable\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Processing pipeline failed - no final dataset available\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No raw data available for processing validation\")\n",
    "    print(\"üí° Please run the previous cells to load raw data first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789c1cf7",
   "metadata": {},
   "source": [
    "## Step 4: Export & Validation of Processed Data\n",
    "\n",
    "Save the processed data in multiple formats and validate the export process. This step ensures the pipeline produces the expected output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea8fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT VALIDATION & FINAL TESTING\n",
    "if 'processor' in locals() and hasattr(processor, 'df_processed') and processor.df_processed is not None:\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"üíæ DATA EXPORT & VALIDATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # STEP 1: Export processed data\n",
    "    print(\"üîÑ Exporting processed data to multiple formats...\")\n",
    "    \n",
    "    try:\n",
    "        # Create a test output directory\n",
    "        test_output_dir = \"../data/validation_output\"\n",
    "        Path(test_output_dir).mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Export using processor method\n",
    "        processor.save_processed_data(processor.df_processed, test_output_dir)\n",
    "        \n",
    "        print(\"‚úÖ Export completed successfully!\")\n",
    "        \n",
    "        # Validate exported files\n",
    "        print(f\"\\nüìÅ EXPORT VALIDATION:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        expected_files = [\n",
    "            \"job_market_processed.parquet\",\n",
    "            \"job_market_sample.csv\", \n",
    "            \"data_schema.json\",\n",
    "            \"processing_report.md\"\n",
    "        ]\n",
    "        \n",
    "        for file_name in expected_files:\n",
    "            file_path = Path(test_output_dir) / file_name\n",
    "            \n",
    "            if file_path.exists():\n",
    "                if file_name.endswith('.parquet'):\n",
    "                    # For parquet, check if it's a directory with files\n",
    "                    if file_path.is_dir():\n",
    "                        parquet_files = list(file_path.glob(\"*.parquet\"))\n",
    "                        success_marker = file_path / \"_SUCCESS\"\n",
    "                        \n",
    "                        if parquet_files and success_marker.exists():\n",
    "                            print(f\"   ‚úÖ {file_name}/ ({len(parquet_files)} parquet files)\")\n",
    "                        else:\n",
    "                            print(f\"   ‚ö†Ô∏è  {file_name}/ (incomplete)\")\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  {file_name} (unexpected file type)\")\n",
    "                        \n",
    "                elif file_name.endswith('.csv'):\n",
    "                    # Check CSV file size\n",
    "                    size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                    print(f\"   ‚úÖ {file_name} ({size_mb:.1f} MB)\")\n",
    "                    \n",
    "                else:\n",
    "                    # Other files\n",
    "                    print(f\"   ‚úÖ {file_name}\")\n",
    "            else:\n",
    "                print(f\"   ‚ùå {file_name} (missing)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Export failed: {e}\")\n",
    "    \n",
    "    # STEP 2: Test data loading from exported files\n",
    "    print(f\"\\n\udd04 TESTING EXPORTED DATA LOADING:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Test loading from exported Parquet\n",
    "        parquet_path = Path(test_output_dir) / \"job_market_processed.parquet\"\n",
    "        \n",
    "        if parquet_path.exists():\n",
    "            print(\"üîÑ Testing Parquet reload...\")\n",
    "            \n",
    "            # Create new analyzer to test loading\n",
    "            test_analyzer = SparkJobAnalyzer()\n",
    "            test_analyzer.load_full_dataset(str(parquet_path))\n",
    "            \n",
    "            # Validate loaded data\n",
    "            test_count = test_analyzer.job_data.count()\n",
    "            test_cols = len(test_analyzer.job_data.columns)\n",
    "            \n",
    "            print(f\"‚úÖ Parquet reload successful!\")\n",
    "            print(f\"   Records: {test_count:,}\")\n",
    "            print(f\"   Columns: {test_cols}\")\n",
    "            \n",
    "            # Quick analysis test\n",
    "            try:\n",
    "                quick_stats = test_analyzer.get_overall_statistics()\n",
    "                print(f\"   Median Salary: ${quick_stats['median_salary']:,}\")\n",
    "                print(f\"‚úÖ Analysis functions working correctly\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Analysis test failed: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Reload test failed: {e}\")\n",
    "    \n",
    "    # STEP 3: Pandas conversion test for visualization\n",
    "    print(f\"\\nüîÑ TESTING PANDAS CONVERSION FOR VISUALIZATION:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    try:\n",
    "        # Convert a sample to Pandas\n",
    "        sample_fraction = 0.05  # 5% sample for testing\n",
    "        pandas_sample = processor.df_processed.sample(fraction=sample_fraction, seed=42).toPandas()\n",
    "        \n",
    "        print(f\"‚úÖ Pandas conversion successful!\")\n",
    "        print(f\"   Sample size: {len(pandas_sample):,} records ({sample_fraction*100}% of total)\")\n",
    "        \n",
    "        # Test SalaryVisualizer initialization\n",
    "        print(f\"üîÑ Testing SalaryVisualizer integration...\")\n",
    "        \n",
    "        # Map columns for visualizer\n",
    "        column_mapping = {\n",
    "            'SALARY_AVG_IMPUTED': 'salary_avg',\n",
    "            'INDUSTRY_CLEAN': 'industry',\n",
    "            'EXPERIENCE_LEVEL_CLEAN': 'experience_level',\n",
    "            'TITLE': 'title',\n",
    "            'LOCATION': 'location'\n",
    "        }\n",
    "        \n",
    "        # Apply column mapping\n",
    "        viz_data = pandas_sample.copy()\n",
    "        mapped_columns = []\n",
    "        \n",
    "        for source_col, target_col in column_mapping.items():\n",
    "            if source_col in viz_data.columns:\n",
    "                viz_data[target_col] = viz_data[source_col]\n",
    "                mapped_columns.append(f\"{source_col} ‚Üí {target_col}\")\n",
    "        \n",
    "        print(f\"   Column mappings applied: {len(mapped_columns)}\")\n",
    "        for mapping in mapped_columns[:3]:  # Show first 3 mappings\n",
    "            print(f\"     ‚úÖ {mapping}\")\n",
    "        \n",
    "        # Test visualizer initialization\n",
    "        if 'salary_avg' in viz_data.columns:\n",
    "            visualizer = SalaryVisualizer(viz_data)\n",
    "            \n",
    "            # Quick visualization test\n",
    "            industry_analysis = visualizer.get_industry_salary_analysis(top_n=5)\n",
    "            print(f\"‚úÖ SalaryVisualizer working correctly!\")\n",
    "            print(f\"   Industry analysis: {len(industry_analysis)} industries\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  Salary column not available for visualization\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Pandas conversion test failed: {e}\")\n",
    "    \n",
    "    # FINAL SUMMARY\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ DEVELOPER VALIDATION COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"‚úÖ Data Processing Pipeline Validated:\")\n",
    "    print(f\"   üîÑ Raw data loading: Success\")\n",
    "    print(f\"   üßπ Data cleaning: Success\") \n",
    "    print(f\"   ‚öôÔ∏è  Feature engineering: Success\")\n",
    "    print(f\"   üíæ Multi-format export: Success\")\n",
    "    print(f\"   üìä Analysis integration: Success\")\n",
    "    print(f\"   üìà Visualization readiness: Success\")\n",
    "    \n",
    "    print(f\"\\nüéØ Available Objects for Further Development:\")\n",
    "    print(f\"   - raw_analyzer: SparkJobAnalyzer with raw data\")\n",
    "    print(f\"   - processor: JobMarketDataProcessor with processed data\")\n",
    "    print(f\"   - test_analyzer: SparkJobAnalyzer with exported data\")\n",
    "    print(f\"   - visualizer: SalaryVisualizer with sample data\")\n",
    "    \n",
    "    print(f\"\\n\udcc1 Exported Files Available in: {test_output_dir}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No processed data available for export validation\")\n",
    "    print(\"üí° Please run the previous processing steps first\")\n",
    "    \n",
    "    # Show what's available for debugging\n",
    "    print(f\"\\n\udd27 Debug Information:\")\n",
    "    if 'raw_analyzer' in locals():\n",
    "        print(f\"   ‚úÖ raw_analyzer available\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå raw_analyzer not available\")\n",
    "        \n",
    "    if 'processor' in locals():\n",
    "        print(f\"   ‚úÖ processor available\")\n",
    "        if hasattr(processor, 'df_processed'):\n",
    "            print(f\"   ‚úÖ processor.df_processed available\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå processor.df_processed not available\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå processor not available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
