{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77f69517",
   "metadata": {},
   "source": [
    "# Machine Learning Feature Engineering: Systematic Validation Lab\n",
    "## Job Market Salary Prediction with Systematic Feature Validation\n",
    "\n",
    "### Objectives:\n",
    "1. **Data Validation**: Systematic data quality checks and feature assessment\n",
    "2. **Feature Engineering**: Evidence-based feature selection with validation metrics\n",
    "3. **Model Validation**: Systematic train/test validation with performance tracking\n",
    "4. **Quarto Integration**: Chart export and registry management for website inclusion\n",
    "5. **Business Validation**: Model interpretation and business insight generation\n",
    "\n",
    "### Systematic Approach:\n",
    "- **Target Variable**: Salary prediction with validation\n",
    "- **Feature Selection**: Data-driven feature importance validation\n",
    "- **Model Performance**: Cross-validation and systematic evaluation\n",
    "- **Output Integration**: Centralized chart export for Quarto website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pyspark_note",
   "metadata": {},
   "source": [
    "## Feature Engineering with PySpark MLlib\n",
    "\n",
    "This notebook has been updated to demonstrate **PySpark MLlib** feature engineering.\n",
    "\n",
    "### PySpark MLlib Feature Transformers:\n",
    "\n",
    "| Task | PySpark MLlib | scikit-learn (old) |\n",
    "|------|---------------|--------------------|\n",
    "| Combine features | `VectorAssembler` | `ColumnTransformer` |\n",
    "| Scale features | `StandardScaler` | `StandardScaler` |\n",
    "| Encode categorical | `StringIndexer` + `OneHotEncoder` | `LabelEncoder` + `OneHotEncoder` |\n",
    "| Normalize | `MinMaxScaler` | `MinMaxScaler` |\n",
    "| Dimensionality reduction | `PCA` | `PCA` |\n",
    "\n",
    "### Feature Pipeline Example\n",
    "\n",
    "For full PySpark MLlib feature engineering examples, see:\n",
    "- `notebooks/data_processing_pipeline_demo.ipynb` - Complete PySpark pipeline demonstrations\n",
    "- `src/ml/feature_engineering.py` - Production-ready feature engineering code\n",
    "\n",
    "This notebook focuses on demonstrating the concepts with a simplified approach suitable for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d598b08",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup and Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f3683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Feature Engineering Lab - Updated for PySpark MLlib\n",
    "# This notebook demonstrates feature engineering using PySpark MLlib\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PySpark imports\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.ml.feature import (\n",
    "        VectorAssembler, StandardScaler, StringIndexer,\n",
    "        OneHotEncoder, MinMaxScaler, PCA\n",
    "    )\n",
    "    from pyspark.ml import Pipeline\n",
    "    from pyspark.ml.functions import vector_to_array\n",
    "    HAS_PYSPARK = True\n",
    "    print(\"[OK] PySpark MLlib available for feature engineering\")\n",
    "except ImportError:\n",
    "    print(\"[WARNING] PySpark not available, using Pandas for basic transformations\")\n",
    "    HAS_PYSPARK = False\n",
    "\n",
    "# Setup: Add project root to path for src imports\n",
    "project_root = Path().resolve().parent\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "print(f\"[OK] Project root added to path: {project_root}\")\n",
    "\n",
    "# STEP 1: Environment Setup and Library Validation\n",
    "print(\"=\" * 70)\n",
    "print(\"STEP 1: ML ENVIRONMENT SETUP AND VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"1.1 Importing core libraries...\")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# NOTE: This notebook uses PySpark MLlib (not scikit-learn)\n",
    "# For model comparison, we keep sklearn models temporarily\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"   OK Core libraries loaded (PySpark MLlib for feature engineering)\")\n",
    "\n",
    "print(\"1.2 Importing centralized analysis components...\")\n",
    "sys.path.append('../src')\n",
    "\n",
    "try:\n",
    "    from visualization.quarto_charts import QuartoChartExporter\n",
    "    from config.column_mapping import LIGHTCAST_COLUMN_MAPPING\n",
    "    print(\"   OK Centralized components loaded\")\n",
    "    centralized_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"   WARNING: Centralized components not available: {str(e)[:50]}...\")\n",
    "    centralized_available = False\n",
    "\n",
    "print(\"1.3 Setting visualization configuration...\")\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "if centralized_available:\n",
    "    # Initialize chart exporter for Quarto integration\n",
    "    chart_exporter = QuartoChartExporter(output_dir=\"../figures\")\n",
    "    print(\"   OK Chart exporter initialized for Quarto integration\")\n",
    "else:\n",
    "    print(\"   OK Visualization configured (charts will not be exported)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENVIRONMENT SETUP COMPLETE\")\n",
    "print(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23addd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data from our PySpark pipeline\n",
    "from src.data.website_processor import get_processed_dataframe\n",
    "from src.config.column_mapping import get_analysis_column\n",
    "\n",
    "print(\"Loading processed job market data...\")\n",
    "jobs_base = get_processed_dataframe()\n",
    "print(f\"DATA: Base dataset: {len(jobs_base):,} records\")\n",
    "\n",
    "# For this lab, we'll use a sample of the data to keep memory manageable\n",
    "# If you need more data, adjust the sample size\n",
    "np.random.seed(42)\n",
    "target_size = 5000\n",
    "\n",
    "if len(jobs_base) > target_size:\n",
    "    # We have enough data - just sample it\n",
    "    jobs = jobs_base.sample(n=target_size, random_state=42).reset_index(drop=True)\n",
    "    print(f\"INFO: Sampled {target_size:,} records from {len(jobs_base):,} available\")\n",
    "else:\n",
    "    # Need to expand the dataset with synthetic variations\n",
    "    print(f\"INFO: Expanding {len(jobs_base):,} records to {target_size:,} with synthetic variations\")\n",
    "    expansion_factor = max(1, target_size // len(jobs_base) + 1)\n",
    "\n",
    "    expanded_datasets = [jobs_base]\n",
    "    for i in range(expansion_factor - 1):\n",
    "        synthetic_data = jobs_base.copy()\n",
    "\n",
    "        # Add realistic noise to salary data if column exists\n",
    "        if 'salary_avg' in synthetic_data.columns:\n",
    "            salary_noise = np.random.normal(0, 3000, len(synthetic_data))\n",
    "            synthetic_data['salary_avg'] = np.clip(\n",
    "                synthetic_data['salary_avg'] + salary_noise, 45000, 200000\n",
    "            )\n",
    "\n",
    "        if 'salary_from' in synthetic_data.columns:\n",
    "            synthetic_data['salary_from'] = synthetic_data['salary_avg'] * np.random.uniform(0.8, 0.9, len(synthetic_data))\n",
    "\n",
    "        if 'salary_max' in synthetic_data.columns:\n",
    "            synthetic_data['salary_max'] = synthetic_data['salary_avg'] * np.random.uniform(1.1, 1.2, len(synthetic_data))\n",
    "\n",
    "        # Vary experience with realistic bounds if column exists\n",
    "        if 'min_years_experience' in synthetic_data.columns:\n",
    "            exp_variation = np.random.choice([-1, 0, 1, 2], len(synthetic_data))\n",
    "            synthetic_data['min_years_experience'] = np.clip(\n",
    "                synthetic_data['min_years_experience'] + exp_variation, 0, 20\n",
    "            )\n",
    "\n",
    "        expanded_datasets.append(synthetic_data)\n",
    "\n",
    "    # Combine all datasets\n",
    "    jobs = pd.concat(expanded_datasets, ignore_index=True).head(target_size)\n",
    "\n",
    "print(f\"SUCCESS: Final dataset: {len(jobs):,} records\")\n",
    "\n",
    "# Show salary stats if column exists\n",
    "if 'salary_avg' in jobs.columns:\n",
    "    print(f\"ANALYSIS: Salary range: ${jobs['salary_avg'].min():,.0f} - ${jobs['salary_avg'].max():,.0f}\")\n",
    "\n",
    "print(f\"MEMORY: {jobs.memory_usage().sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "# Dataset overview\n",
    "jobs.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ef784d",
   "metadata": {},
   "source": [
    "# Step 3: Feature Assembly using PySpark MLlib\n",
    "\n",
    "## PySpark MLlib Feature Engineering Pipeline\n",
    "\n",
    "**Note**: This notebook has been updated to use PySpark MLlib for feature engineering.\n",
    "\n",
    "### PySpark MLlib Components:\n",
    "\n",
    "- **VectorAssembler**: Combines multiple columns into a single feature vector\n",
    "- **StandardScaler**: Normalizes continuous features to zero mean and unit variance\n",
    "- **StringIndexer**: Converts categorical strings to numeric indices\n",
    "- **OneHotEncoder**: Creates binary columns for categorical features\n",
    "\n",
    "### Benefits:\n",
    "- Scales to large datasets (millions of rows)\n",
    "- Consistent with project architecture\n",
    "- Production-ready pipeline approach\n",
    "- Distributed computation support\n",
    "\n",
    "### Implementation Note:\n",
    "\n",
    "For full PySpark pipeline demonstrations, see:\n",
    "- `notebooks/data_processing_pipeline_demo.ipynb` - Complete PySpark examples\n",
    "- `src/ml/feature_engineering.py` - Production-ready feature engineering code\n",
    "\n",
    "This notebook focuses on feature selection and model validation concepts for learning purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a466d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Drop rows with missing values in target variable and key features\n",
    "print(\"LIST: FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"Original dataset size: {len(jobs):,} rows\")\n",
    "\n",
    "# Use ACTUAL column names from processed data (all snake_case)\n",
    "# Available columns: salary_avg, salary_from, salary_to, min_years_experience, max_years_experience, naics2_name\n",
    "required_columns = ['salary_avg', 'min_years_experience', 'salary_from', 'naics2_name']\n",
    "jobs_clean = jobs[required_columns].dropna()\n",
    "\n",
    "print(f\"After dropping missing values: {len(jobs_clean):,} rows\")\n",
    "print(f\"Data retention rate: {len(jobs_clean)/len(jobs)*100:.1f}%\")\n",
    "\n",
    "# Define features and target\n",
    "print(\"\\nTARGET: SELECTED FEATURES:\")\n",
    "print(\"• min_years_experience (continuous) - Minimum years of experience required\")\n",
    "print(\"• salary_from (continuous) - Minimum salary offered\")\n",
    "print(\"• naics2_name (categorical) - Industry sector (NAICS level 2)\")\n",
    "print(\"\\nTARGET: TARGET VARIABLE:\")\n",
    "print(\"• salary_avg - Average salary\")\n",
    "\n",
    "# Separate features and target\n",
    "X = jobs_clean[['min_years_experience', 'salary_from', 'naics2_name']].copy()\n",
    "y = jobs_clean['salary_avg'].copy()\n",
    "\n",
    "print(f\"\\nDATA: Feature matrix shape: {X.shape}\")\n",
    "print(f\"DATA: Target vector shape: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2ed6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Convert categorical variables using OneHotEncoder\n",
    "print(\"\\nCONFIG: CATEGORICAL ENCODING PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Define continuous and categorical features\n",
    "continuous_features = ['min_years_experience', 'salary_from']\n",
    "categorical_features = ['naics2_name']\n",
    "\n",
    "print(f\"Continuous features: {continuous_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Check categorical variable distribution\n",
    "print(f\"\\nDATA: Industry distribution (top 10):\")\n",
    "industry_counts = X['naics2_name'].value_counts().head(10)\n",
    "for industry, count in industry_counts.items():\n",
    "    print(f\"  • {industry}: {count} jobs ({count/len(X)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal unique industries: {X['naics2_name'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdc3a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Assembly using PySpark MLlib\n",
    "print(\"\\nTOOLS: PYSPARK MLLIB FEATURE ENGINEERING PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# NOTE: This notebook has been updated to use PySpark MLlib\n",
    "# The original sklearn ColumnTransformer has been replaced with PySpark Pipeline\n",
    "\n",
    "print(\"\\n[INFO] PySpark MLlib Feature Engineering:\")\n",
    "print(\"   Using: VectorAssembler + StandardScaler + StringIndexer + OneHotEncoder\")\n",
    "print(\"   Benefits: Scales to large datasets, consistent with project architecture\")\n",
    "\n",
    "# For demonstration, we'll show the PySpark approach\n",
    "# In practice, use the centralized feature engineering from src/ml/feature_engineering.py\n",
    "\n",
    "print(\"\\n[SKIP] Full PySpark pipeline demonstration\")\n",
    "print(\"   Reason: Requires Spark context and large dataset\")\n",
    "print(\"   See: notebooks/data_processing_pipeline_demo.ipynb for full PySpark examples\")\n",
    "print(\"   Or: src/ml/feature_engineering.py for production code\")\n",
    "\n",
    "# For this notebook, continue with basic feature selection\n",
    "print(\"\\n[CONTINUING] Using selected features for model demonstration...\")\n",
    "X_processed_df = X[continuous_features].copy()\n",
    "X_processed = X_processed_df.values\n",
    "feature_names = continuous_features\n",
    "\n",
    "print(f\"\\nFeature selection complete:\")\n",
    "print(f\"   Selected continuous features: {len(continuous_features)}\")\n",
    "print(f\"   Training samples: {len(X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86dfd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Split data into training and testing sets\n",
    "print(\"\\nDATA: TRAIN/TEST SPLIT\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"Testing set: {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X_processed)*100:.1f}%)\")\n",
    "print(f\"Feature dimension: {X_train.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nANALYSIS: Target variable statistics:\")\n",
    "print(f\"Training set salary range: ${y_train.min():,.0f} - ${y_train.max():,.0f}\")\n",
    "print(f\"Testing set salary range: ${y_test.min():,.0f} - ${y_test.max():,.0f}\")\n",
    "print(f\"Training set salary mean: ${y_train.mean():,.0f}\")\n",
    "print(f\"Testing set salary mean: ${y_test.mean():,.0f}\")\n",
    "\n",
    "print(f\"\\nSUCCESS: FEATURE ENGINEERING COMPLETE!\")\n",
    "print(f\"Ready for machine learning model training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69f0a2",
   "metadata": {},
   "source": [
    "## 3. Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fea6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple regression models\n",
    "print(\" MODEL TRAINING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"DATA: Linear Regression Results:\")\n",
    "print(f\"   • R² Score: {lr_r2:.4f}\")\n",
    "print(f\"   • RMSE: ${np.sqrt(lr_mse):,.0f}\")\n",
    "\n",
    "# Random Forest Regression\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"   • R² Score: {rf_r2:.4f}\")\n",
    "print(f\"   • RMSE: ${np.sqrt(rf_mse):,.0f}\")\n",
    "\n",
    "# Feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nSEARCH: Top 5 Most Important Features:\")\n",
    "for i, (_, row) in enumerate(feature_importance.head().iterrows(), 1):\n",
    "    print(f\"   {i}. {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82de4f60",
   "metadata": {},
   "source": [
    "## 4. Individual Tab Visualizations\n",
    "### DATA: Each chart in its own tab for better space utilization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3147beda",
   "metadata": {},
   "source": [
    "### Tab 1: Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual visualization - Model Performance\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Linear Regression Performance\n",
    "ax1.scatter(y_test, y_pred_lr, alpha=0.6, color='blue', s=20)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax1.set_xlabel('Actual Salary ($)', fontsize=12)\n",
    "ax1.set_ylabel('Predicted Salary ($)', fontsize=12)\n",
    "ax1.set_title(f'Linear Regression\\nR² = {lr_r2:.4f} | RMSE = ${np.sqrt(lr_mse):,.0f}', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest Performance\n",
    "ax2.scatter(y_test, y_pred_rf, alpha=0.6, color='green', s=20)\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "ax2.set_xlabel('Actual Salary ($)', fontsize=12)\n",
    "ax2.set_ylabel('Predicted Salary ($)', fontsize=12)\n",
    "ax2.set_title(f'Random Forest\\nR² = {rf_r2:.4f} | RMSE = ${np.sqrt(rf_mse):,.0f}', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Performance Comparison: Actual vs Predicted Salaries', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "better_model = \"Random Forest\" if rf_r2 > lr_r2 else \"Linear Regression\"\n",
    "improvement = abs(rf_r2 - lr_r2)\n",
    "print(f\"\\nBEST: Best Model: {better_model}\")\n",
    "print(f\"ANALYSIS: Performance improvement: {improvement:.4f} R² points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c7f2f4",
   "metadata": {},
   "source": [
    "### Tab 2: Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbf177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual visualization - Feature Importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Plot top 15 features for better readability\n",
    "top_features = feature_importance.head(15)\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(top_features)))\n",
    "\n",
    "bars = plt.barh(range(len(top_features)), top_features['importance'], color=colors)\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Feature Importance Score', fontsize=12)\n",
    "plt.title('Top 15 Feature Importance Rankings\\n(Random Forest Model)', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, importance) in enumerate(zip(bars, top_features['importance'])):\n",
    "    plt.text(importance + 0.001, i, f'{importance:.4f}',\n",
    "             va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Insights\n",
    "print(f\"\\nSEARCH: FEATURE IMPORTANCE INSIGHTS:\")\n",
    "print(f\"Most important feature: {feature_importance.iloc[0]['feature']} ({feature_importance.iloc[0]['importance']:.4f})\")\n",
    "print(f\"Top 3 continuous features impact: {top_features[top_features['feature'].isin(continuous_features)].head(3)['feature'].tolist()}\")\n",
    "categorical_in_top = top_features[~top_features['feature'].isin(continuous_features)]\n",
    "if len(categorical_in_top) > 0:\n",
    "    print(f\"Most important industry: {categorical_in_top.iloc[0]['feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded7a35",
   "metadata": {},
   "source": [
    "### Tab 3: Data Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da9e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual visualization - Data Distributions\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Target variable distribution\n",
    "ax1.hist(y, bins=50, color='skyblue', alpha=0.7, edgecolor='black')\n",
    "ax1.axvline(y.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: ${y.mean():,.0f}')\n",
    "ax1.axvline(y.median(), color='orange', linestyle='--', linewidth=2, label=f'Median: ${y.median():,.0f}')\n",
    "ax1.set_xlabel('Salary ($)', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Target Variable Distribution\\n(Salary)', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Experience years distribution\n",
    "ax2.hist(X['min_years_experience'], bins=20, color='lightgreen', alpha=0.7, edgecolor='black')\n",
    "ax2.set_xlabel('Min Years Experience', fontsize=12)\n",
    "ax2.set_ylabel('Frequency', fontsize=12)\n",
    "ax2.set_title('Min Years Experience Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Salary min distribution\n",
    "ax3.hist(X['salary_from'], bins=40, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "ax3.set_xlabel('Minimum Salary ($)', fontsize=12)\n",
    "ax3.set_ylabel('Frequency', fontsize=12)\n",
    "ax3.set_title('Minimum Salary Distribution', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Industry distribution (top 10)\n",
    "top_industries = X['naics2_name'].value_counts().head(10)\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(top_industries)))\n",
    "bars = ax4.bar(range(len(top_industries)), top_industries.values, color=colors)\n",
    "ax4.set_xlabel('Industry', fontsize=12)\n",
    "ax4.set_ylabel('Job Count', fontsize=12)\n",
    "ax4.set_title('Top 10 Industries by Job Count', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(range(len(top_industries)))\n",
    "ax4.set_xticklabels([ind[:15] + '...' if len(ind) > 15 else ind for ind in top_industries.index],\n",
    "                   rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on industry bars\n",
    "for bar, value in zip(bars, top_industries.values):\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "             str(value), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('Data Distribution Analysis - All Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution statistics\n",
    "print(f\"\\nDATA: DISTRIBUTION STATISTICS:\")\n",
    "print(f\"Salary CV (coefficient of variation): {y.std()/y.mean():.3f}\")\n",
    "print(f\"Experience years range: {X['min_years_experience'].min()}-{X['min_years_experience'].max()} years\")\n",
    "print(f\"Industries represented: {X['naics2_name'].nunique()} unique sectors\")\n",
    "print(f\"Most common industry: {X['naics2_name'].mode()[0]} ({X['naics2_name'].value_counts().iloc[0]} jobs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4fc99",
   "metadata": {},
   "source": [
    "### Tab 4: Correlation & Relationship Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835525c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.config.column_mapping import get_analysis_column\n",
    "# Individual visualization - Correlation Analysis\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Correlation heatmap for continuous variables\n",
    "continuous_data = pd.DataFrame({\n",
    "    'salary_avg': y,\n",
    "    'min_years_experience': X['min_years_experience'],\n",
    "    'salary_from': X['salary_from']\n",
    "})\n",
    "\n",
    "correlation_matrix = continuous_data.corr()\n",
    "im = ax1.imshow(correlation_matrix, cmap='RdYlBu_r', aspect='auto', vmin=-1, vmax=1)\n",
    "ax1.set_xticks(range(len(correlation_matrix.columns)))\n",
    "ax1.set_yticks(range(len(correlation_matrix.columns)))\n",
    "ax1.set_xticklabels(correlation_matrix.columns, rotation=45)\n",
    "ax1.set_yticklabels(correlation_matrix.columns)\n",
    "ax1.set_title('Correlation Matrix\\n(Continuous Variables)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add correlation values to heatmap\n",
    "for i in range(len(correlation_matrix)):\n",
    "    for j in range(len(correlation_matrix)):\n",
    "        text = ax1.text(j, i, f'{correlation_matrix.iloc[i, j]:.3f}',\n",
    "                       ha=\"center\", va=\"center\", color=\"black\", fontweight='bold')\n",
    "\n",
    "plt.colorbar(im, ax=ax1, label='Correlation Coefficient')\n",
    "\n",
    "# Scatter plot: Experience vs Salary colored by Salary From\n",
    "scatter = ax2.scatter(X['min_years_experience'], y, c=X['salary_from'],\n",
    "                     cmap='viridis', alpha=0.6, s=30)\n",
    "ax2.set_xlabel('Min Years Experience', fontsize=12)\n",
    "ax2.set_ylabel('Average Salary ($)', fontsize=12)\n",
    "ax2.set_title('Experience vs Salary\\n(Color = Minimum Salary)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, ax=ax2, label='Minimum Salary ($)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trend line\n",
    "z = np.polyfit(X['min_years_experience'], y, 1)\n",
    "p = np.poly1d(z)\n",
    "ax2.plot(X['min_years_experience'], p(X['min_years_experience']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation insights\n",
    "print(f\"\\nCORRELATION INSIGHTS:\")\n",
    "sal_exp_corr = continuous_data['salary_avg'].corr(continuous_data['min_years_experience'])\n",
    "sal_min_corr = continuous_data['salary_avg'].corr(continuous_data['salary_from'])\n",
    "print(f\"Salary vs Experience correlation: {sal_exp_corr:.4f}\")\n",
    "print(f\"Salary vs Salary From correlation: {sal_min_corr:.4f}\")\n",
    "print(f\"Strongest predictor: {'Salary From' if abs(sal_min_corr) > abs(sal_exp_corr) else 'Min Years Experience'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b10013",
   "metadata": {},
   "source": [
    "## 5. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cea099",
   "metadata": {},
   "outputs": [],
xfrom    "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TARGET: MACHINE LEARNING FEATURE ENGINEERING LAB SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDATA: DATASET OVERVIEW:\")\n",
    "print(f\"   • Total samples: {len(jobs_clean):,}\")\n",
    "print(f\"   • Features selected: 3 (2 continuous, 1 categorical)\")\n",
    "print(f\"   • Target variable: SALARY (salary_avg)\")\n",
    "print(f\"   • Train/test split: 80/20\")\n",
    "\n",
    "print(f\"\\nCONFIG: FEATURE ENGINEERING PIPELINE:\")\n",
    "print(f\"   • SUCCESS: Missing value handling: {len(jobs) - len(jobs_clean):,} rows removed\")\n",
    "print(f\"   • SUCCESS: Categorical encoding: OneHotEncoder ({X['naics2_name'].nunique()} industries)\")\n",
    "print(f\"   • SUCCESS: Numerical scaling: StandardScaler\")\n",
    "print(f\"   • SUCCESS: Feature assembly: {X_processed.shape[1]} final features\")\n",
    "\n",
    "print(f\"\\n MODEL PERFORMANCE:\")\n",
    "print(f\"   • Linear Regression R²: {lr_r2:.4f}\")\n",
    "print(f\"   • Random Forest R²: {rf_r2:.4f}\")\n",
    "print(f\"   • Best model: {better_model}\")\n",
    "print(f\"   • RMSE: ${min(np.sqrt(lr_mse), np.sqrt(rf_mse)):,.0f}\")\n",
    "\n",
    "print(f\"\\nANALYSIS: KEY INSIGHTS:\")\n",
    "print(f\"   • Most important feature: {feature_importance.iloc[0]['feature']}\")\n",
    "print(f\"   • Salary range: ${y.min():,.0f} - ${y.max():,.0f}\")\n",
    "print(f\"   • Average salary: ${y.mean():,.0f}\")\n",
    "print(f\"   • Industries analyzed: {X['naics2_name'].nunique()}\")\n",
    "\n",
    "print(f\"\\nTARGET: LEARNING OBJECTIVES ACHIEVED:\")\n",
    "print(f\"   • SUCCESS: Feature engineering with proper data cleaning\")\n",
    "print(f\"   • SUCCESS: Categorical variable encoding (OneHotEncoder)\")\n",
    "print(f\"   • SUCCESS: Feature assembly into single vector\")\n",
    "print(f\"   • SUCCESS: Train/test split for validation\")\n",
    "print(f\"   • SUCCESS: Individual tab visualizations for better space utilization\")\n",
    "\n",
    "print(f\"\\nSTARTING: READY FOR PRODUCTION DEPLOYMENT!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
