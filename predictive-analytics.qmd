---
title: "Predictive Analytics"
subtitle: "Machine Learning Insights for Job Market Analysis"
format:
  html:
    theme: cosmo
    toc: true
    code-fold: true
    code-tools: true
    embed-resources: true
    css: styles.css
execute:
  echo: false
  warning: false
  message: false
---

## Overview

This page demonstrates basic predictive analytics using the job market data. We focus on simple, interpretable models that provide actionable insights for job seekers.

```{python}
#| label: data-loading

# Load processed data using centralized pipeline
from src.data import load_analysis_data
from src.config.column_mapping import get_analysis_column
import pandas as pd
import numpy as np

print("Loading data...")
df = load_analysis_data("predictive")

# Get standardized column names
salary_col = get_analysis_column('salary')
city_col = get_analysis_column('city')

print(f"[OK] Loaded {len(df):,} records")
print(f"   Salary column: {salary_col}")
print(f"   Location column: {city_col}")
```

## Salary Distribution Analysis

Understanding the overall salary landscape helps set realistic expectations.

```{python}
#| label: salary-distribution

import plotly.express as px
import plotly.graph_objects as go

# Create salary distribution visualization
fig = px.histogram(
    df,
    x=salary_col,
    nbins=50,
    title="Salary Distribution in Tech Job Market",
    labels={salary_col: "Annual Salary ($)"},
    color_discrete_sequence=['#3498db']
)

# Add median line
median_salary = df[salary_col].median()
fig.add_vline(
    x=median_salary,
    line_dash="dash",
    line_color="red",
    annotation_text=f"Median: ${median_salary:,.0f}",
    annotation_position="top"
)

fig.update_layout(
    height=500,
    showlegend=False,
    xaxis_title="Annual Salary ($)",
    yaxis_title="Number of Jobs"
)

fig.show()

# Print key statistics
print(f"\n[DATA] Salary Statistics:")
print(f"   Median: ${median_salary:,.0f}")
print(f"   Mean: ${df[salary_col].mean():,.0f}")
print(f"   25th percentile: ${df[salary_col].quantile(0.25):,.0f}")
print(f"   75th percentile: ${df[salary_col].quantile(0.75):,.0f}")
```

## Geographic Salary Analysis

Where you work significantly impacts compensation.

```{python}
#| label: geographic-analysis

# Use abstraction layer for geographic analysis
from src.visualization import SalaryVisualizer

visualizer = SalaryVisualizer(df)
fig = visualizer.plot_salary_by_category(city_col)
fig.update_layout(title="Top Cities by Median Salary", height=600)
fig.show()
```

## Simple Salary Prediction

We can predict salary ranges based on key factors using PySpark MLlib.

```{python}
#| label: simple-prediction

from pyspark.sql import SparkSession
from pyspark.ml.regression import LinearRegression
from pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.evaluation import RegressionEvaluator

# Initialize Spark session
spark = SparkSession.builder \
    .appName("Salary Prediction") \
    .master("local[*]") \
    .config("spark.driver.memory", "2g") \
    .getOrCreate()

# Filter out rows with null salaries FIRST
print(f"\n[DATA] Filtering for valid salaries:")
print(f"   Total records: {len(df):,}")
valid_salary_mask = df[salary_col].notna()
df_model = df[valid_salary_mask].copy()
print(f"   With valid salary: {len(df_model):,}")
print(f"   Filtered out: {len(df) - len(df_model):,}")

# Convert to Spark DataFrame
spark_df = spark.createDataFrame(df_model)

# Prepare feature columns
feature_cols = []

# Use top 10 cities as categorical features
if city_col in spark_df.columns:
    print(f"\n[FEATURE] Using {city_col} as categorical feature")
    city_indexer = StringIndexer(inputCol=city_col, outputCol="city_index", handleInvalid="keep")
    city_encoder = OneHotEncoder(inputCol="city_index", outputCol="city_vec")
    feature_cols.append("city_vec")

# Use title if available
if 'title' in spark_df.columns:
    print(f"[FEATURE] Using title as categorical feature")
    title_indexer = StringIndexer(inputCol="title", outputCol="title_index", handleInvalid="keep")
    title_encoder = OneHotEncoder(inputCol="title_index", outputCol="title_vec")
    feature_cols.append("title_vec")

# Use industry if available
if 'naics2_name' in spark_df.columns:
    print(f"[FEATURE] Using industry as categorical feature")
    industry_indexer = StringIndexer(inputCol="naics2_name", outputCol="industry_index", handleInvalid="keep")
    industry_encoder = OneHotEncoder(inputCol="industry_index", outputCol="industry_vec")
    feature_cols.append("industry_vec")

# Assemble features
assembler = VectorAssembler(inputCols=feature_cols, outputCol="features", handleInvalid="skip")

# Create pipeline stages
stages = []
if city_col in spark_df.columns:
    stages.extend([city_indexer, city_encoder])
if 'title' in spark_df.columns:
    stages.extend([title_indexer, title_encoder])
if 'naics2_name' in spark_df.columns:
    stages.extend([industry_indexer, industry_encoder])
stages.append(assembler)

# Build and fit pipeline
pipeline = Pipeline(stages=stages)
pipeline_model = pipeline.fit(spark_df)
transformed_df = pipeline_model.transform(spark_df)

# Split into train/test
train_df, test_df = transformed_df.randomSplit([0.8, 0.2], seed=42)

# Train linear regression model
lr = LinearRegression(featuresCol="features", labelCol=salary_col, maxIter=10)
lr_model = lr.fit(train_df)

# Make predictions
predictions = lr_model.transform(test_df)

# Evaluate
evaluator = RegressionEvaluator(labelCol=salary_col, predictionCol="prediction", metricName="r2")
r2 = evaluator.evaluate(predictions)

evaluator_mae = RegressionEvaluator(labelCol=salary_col, predictionCol="prediction", metricName="mae")
mae = evaluator_mae.evaluate(predictions)

print(f"\n[MODEL] PySpark MLlib Linear Regression Results:")
print(f"   RÂ² Score: {r2:.3f} (explains {r2*100:.1f}% of variance)")
print(f"   Mean Absolute Error: ${mae:,.0f}")
print(f"   Training samples: {train_df.count():,}")
print(f"   Test samples: {test_df.count():,}")

# Show feature importance
print(f"\n[INFO] Model trained with {len(feature_cols)} feature groups:")
for feat in feature_cols:
    print(f"   - {feat}")

# Stop Spark
spark.stop()
```

## Key Insights for Job Seekers

Based on the analysis above, here are actionable recommendations:

### [TARGET] Location Strategy
- **Highest paying markets** are visible in the geographic analysis above
- Consider **cost of living** alongside salary when evaluating offers
- **Remote opportunities** may offer flexibility while maintaining competitive salaries

### ðŸ’¼ Career Planning
- **Experience matters**: Salary generally increases with career progression
- **Specialization pays**: Certain skills and titles command premium compensation
- **Market awareness**: Understanding salary ranges helps in negotiations

### [DATA] Using This Analysis
1. **Benchmark your offers** against median salaries in your target location
2. **Identify high-opportunity markets** for relocation or remote work
3. **Plan skill development** in areas that show strong salary correlation

---

```{python}
#| label: footer-text
#| output: asis

from IPython.display import Markdown, display
display(Markdown(f"*Analysis based on {len(df):,} real job postings. Data is continuously updated to reflect current market conditions.*"))
```
