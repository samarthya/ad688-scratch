---
title: "Tech Career Intelligence"
subtitle: "Data-Driven Job Market Analysis with PySpark & Machine Learning"
author:
  - name: Saurabh Sharma
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
  - name: Haocun Wang
    affiliations:
      - id: bu
        name: Boston University
        city: Boston
        state: MA
date: today
format:
  revealjs:
    theme: [default, custom-presentation.scss]
    slide-number: true
    chalkboard: true
    preview-links: auto
    logo: ""
    footer: "Tech Career Intelligence | BU AD688"
    transition: slide
    background-transition: fade
    code-fold: true
    code-tools: true
    smaller: false
    scrollable: true
  docx:
    toc: true
    number-sections: true
    highlight-style: github
---

```{python}
#| echo: false
#| include: false

# Setup for presentation
import os
import sys

# Import display_figure from visualization module
from src.visualization.charts import display_figure

# Alias for presentation (keep existing code working)
render_figure = display_figure
```

## Introduction: The Problem {.center}

> I'm graduating soon. Should I get my Master's or start working?
> Where should I apply?
> How much should I ask for?

---

## The Reality {.center}

Most career decisions are based on:

- Anecdotes from friends
- Outdated salary surveys
- Gut feeling

### What if we had DATA instead?

---

## Thesis Statement {.center}

### **Data-driven career intelligence can transform how students and professionals navigate the tech job market**

---

## Research Questions {.center}

By analyzing **72,000+ real job postings** with modern data science tools, we can answer:

1. **Experience Premium:** How much more will I earn as I gain experience?
2. **Education ROI:** Is graduate school financially worth it?
3. **Location Strategy:** Where should I work to maximize earning potential?

---

## Our Approach {.center}

**Combine scalable data processing (PySpark), machine learning (MLlib), and interactive visualization (Plotly/Quarto) to provide evidence-based career guidance.**

- 13M rows ‚Üí 72K clean records
- Multiple ML models (regression, classification)
- Interactive dashboards
- Reproducible analysis

---

## Presentation Roadmap {.smaller}

### What You'll Learn Today

:::: {.columns}

::: {.column width="50%"}
**Part 1: The Research Question** (5 min)

- What problem are we solving?
- Why does this matter?
- What data did we use?

**Part 2: Key Insights** (8 min)

- Experience premium analysis
- Education ROI calculation
- Geographic intelligence
:::

::: {.column width="50%"}
**Part 3: Technical Approach** (8 min)

- PySpark architecture
- Machine learning models
- NLP skills analysis

**Part 4: Impact & Lessons** (4 min)

- What this means for job-seekers
- Technical lessons learned
- Future directions
:::

::::

---

## The Challenge {.center}

### How can students make **data-driven** career decisions?

:::: {.columns}

::: {.column width="33%"}
**Question 1**

"How much will I earn?"

Entry ‚Üí Senior progression?
:::

::: {.column width="33%"}
**Question 2**

"Is grad school worth it?"

Masters ROI calculation?
:::

::: {.column width="33%"}
**Question 3**

"Where should I work?"

Location vs. salary tradeoffs?
:::

::::

::: {.notes}
Students face critical career decisions without data. This project provides evidence-based answers.
:::

---

## Dataset & Architecture {.smaller}

### Data Pipeline: Raw ‚Üí Processed ‚Üí Insights

```{mermaid}
%%| fig-width: 10
graph LR
    A[Raw CSV<br/>13M rows<br/>131 columns] -->|PySpark ETL<br/>5-10 min| B[Parquet<br/>72K rows<br/>132 columns]
    B -->|PySpark Load<br/>1-2 sec| C[Analysis<br/>Statistics<br/>Aggregations]
    B -->|PySpark MLlib<br/>30 sec| D[ML Models<br/>Regression<br/>Classification]
    C --> E[Plotly Charts]
    D --> E
    E -->|Quarto| F[Interactive<br/>Website]

    style A fill:#e3f2fd
    style B fill:#c8e6c9
    style C fill:#fff9c4
    style D fill:#ffccbc
    style E fill:#f8bbd0
    style F fill:#d1c4e9
```

**Key Design Decision:** End-to-end PySpark for ETL, analysis, and ML

---

## Key Finding #1: Experience Premium {.smaller}

```{python}
#| echo: false
#| warning: false

import sys
from pathlib import Path
project_root = Path().resolve()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

from src.data.website_processor import get_processed_dataframe
import plotly.graph_objects as go
import pandas as pd

df = get_processed_dataframe()

# Experience progression analysis
experience_bins = [0, 2, 7, 15, 50]
experience_labels = ['Entry (0-2yr)', 'Mid (3-7yr)', 'Senior (8-15yr)', 'Executive (15+yr)']

df_exp = df[df['salary_avg'].notna() & df['min_years_experience'].notna()].copy()
df_exp['exp_level'] = pd.cut(df_exp['min_years_experience'], bins=experience_bins, labels=experience_labels, include_lowest=True)

exp_stats = df_exp.groupby('exp_level')['salary_avg'].agg(['mean', 'median', 'count']).reset_index()

fig = go.Figure()

fig.add_trace(go.Bar(
    x=exp_stats['exp_level'],
    y=exp_stats['median'],
    name='Median Salary',
    marker_color='#1f77b4',
    text=[f'${v:,.0f}' for v in exp_stats['median']],
    textposition='outside'
))

fig.add_trace(go.Scatter(
    x=exp_stats['exp_level'],
    y=exp_stats['mean'],
    name='Mean Salary',
    mode='lines+markers',
    line=dict(color='#ff7f0e', width=3),
    marker=dict(size=12)
))

fig.update_layout(
    title='Salary Progression by Experience Level',
    xaxis_title='Experience Level',
    yaxis_title='Salary ($)',
    height=400,
    showlegend=True,
    plot_bgcolor='white',
    font=dict(size=14)
)

render_figure(fig, "experience_progression")
```

**Key Insight:** 3.0x growth from Entry ($65K) to Executive ($150K+)

::: {.notes}
Story: Imagine you're Sarah, graduating with a CS degree. She starts at $65K. Fast forward 15 years - she's now making $150K+. That's not luck - that's the power of experience in tech. But here's the question: How do we get there?
:::

---

## What This Means for Job-Seekers {.smaller}

### Real-World Career Strategy

:::: {.columns}

::: {.column width="50%"}
**Scenario: Fresh Graduate**

- **Starting point:** Entry level ($65K)
- **5-year goal:** Mid-level ($85K)
- **10-year goal:** Senior ($120K)

**Action items:**

- Focus on skill development (years 0-3)
- Seek mentorship & complex projects
- Consider job changes for faster growth
- **Expected growth:** +$55K over 10 years
:::

::: {.column width="50%"}
**The "Career Compounding" Effect**

Just like compound interest:

- **Year 1-2:** Learn fundamentals (+10-15% raises)
- **Year 3-5:** Apply & innovate (+15-20% raises)
- **Year 6-10:** Lead & mentor (+20-30% jumps)
- **Year 10+:** Executive track (2-3x increases)

**The Math:** A 15% annual increase on $65K = $262K by year 20!

**Real insight:** Experience isn't just time - it's accumulated problem-solving ability that employers pay premium for.
:::

::::

::: {.notes}
The data shows experience isn't linear - there are inflection points. The biggest jump? Mid to Senior level. That's when you become a force multiplier.
:::

---

## Key Finding #2: Education ROI {.smaller}

:::: {.columns}

::: {.column width="60%"}
```{python}
#| echo: false
#| warning: false

# Education analysis - parse JSON arrays from education_levels_name
df_edu = df[df['salary_avg'].notna() & df['education_levels_name'].notna()].copy()

# Extract primary education level (first in list)
import json

def extract_primary_education(edu_str):
    try:
        if pd.isna(edu_str):
            return None
        edu_list = json.loads(edu_str.replace('\n', ''))
        if edu_list and len(edu_list) > 0:
            return edu_list[0]
    except:
        return None
    return None

df_edu['primary_education'] = df_edu['education_levels_name'].apply(extract_primary_education)

# Map to simplified categories
education_mapping = {
    "High School Diploma": "High School",
    "High school or GED": "High School",
    "Associate degree": "Associate's",
    "Bachelor's degree": "Bachelor's",
    "Master's degree": "Master's",
    "Doctorate": "PhD",
    "Ph.D. or professional degree": "PhD",
    "No Education Listed": None
}

df_edu['education_clean'] = df_edu['primary_education'].map(education_mapping)
df_edu = df_edu[df_edu['education_clean'].notna()]

# Calculate statistics
edu_stats = df_edu.groupby('education_clean')['salary_avg'].agg(['median', 'count']).reset_index()
edu_stats = edu_stats[edu_stats['count'] >= 500].sort_values('median')

if len(edu_stats) > 0:
    baseline = edu_stats[edu_stats['education_clean'] == "Bachelor's"]['median'].iloc[0] if "Bachelor's" in edu_stats['education_clean'].values else edu_stats['median'].median()
    edu_stats['premium_pct'] = ((edu_stats['median'] / baseline - 1) * 100).round(1)

    fig = go.Figure()

    colors = ['#d62728' if p < 0 else '#2ca02c' for p in edu_stats['premium_pct']]

    fig.add_trace(go.Bar(
        y=edu_stats['education_clean'],
        x=edu_stats['premium_pct'],
        orientation='h',
        marker_color=colors,
        text=[f'{p:+.1f}%' for p in edu_stats['premium_pct']],
        textposition='outside'
    ))

    fig.update_layout(
        title='Education Premium vs. Bachelor\'s Degree',
        xaxis_title='Salary Premium (%)',
        yaxis_title='',
        height=350,
        plot_bgcolor='white',
        font=dict(size=12)
    )

    render_figure(fig, "education_premium")
else:
    print("Education data available but sample size too small for reliable analysis")
```
:::

::: {.column width="40%"}
### ROI Calculation

**The $100K Question**

Meet Alex: Bachelor's degree, earning $114K/year

**Option A: Work**

- Keep earning $114K
- 2-year total: $228K earned

**Option B: Master's**

- Cost: $50K tuition + $228K foregone = $278K
- New salary: $137K (+20%)
- Extra per year: $23K
- **Breakeven: 12 years**

**The Real Math:** Master's isn't about immediate ROI - it's about:

- Career ceiling (some roles require it)
- Network & opportunities
- Personal fulfillment
- Long-term compound growth
:::

::::

::: {.notes}
The data shows a 20% premium, but let's be honest - that's not the whole story. For some careers (ML engineering, research), a Master's opens doors that experience alone can't. For others (web dev, SWE), experience wins.
:::

---

## Key Finding #3: Geographic Intelligence {.smaller}

```{python}
#| echo: false
#| warning: false

# Top 10 cities by median salary
if 'city_name' in df.columns:
    city_stats = df[df['salary_avg'].notna()].groupby('city_name')['salary_avg'].agg(['median', 'count']).reset_index()
    city_stats = city_stats[city_stats['count'] >= 50].sort_values('median', ascending=False).head(10)

    fig = go.Figure()

    fig.add_trace(go.Bar(
        y=city_stats['city_name'][::-1],
        x=city_stats['median'][::-1],
        orientation='h',
        marker=dict(
            color=city_stats['median'][::-1],
            colorscale='Viridis',
            showscale=True
        ),
        text=[f'${v:,.0f}' for v in city_stats['median'][::-1]],
        textposition='outside'
    ))

    fig.update_layout(
        title='Top 10 Cities by Median Salary',
        xaxis_title='Median Salary ($)',
        yaxis_title='',
        height=450,
        plot_bgcolor='white',
        font=dict(size=12)
    )

    render_figure(fig, "geographic_analysis")
else:
    print("City data not available")
```

::: {.notes}
Geography matters! Someone in San Francisco making $140K might have the same lifestyle as someone in Austin making $90K after rent/taxes. But here's the kicker - remote work changed everything.
:::

---

## The Remote Work Revolution {.center}

### **2019:** "You need to move to Silicon Valley"
### **2025:** "Work from anywhere"

---

## What Changed? {.center}

**Remote Work Statistics:**

- 45% of tech jobs now fully remote
- 30% offer hybrid options
- Geographic salary gaps narrowing

**The New Strategy:** Find high-paying remote job, live in lower cost area

**Example:** $130K remote job in Austin = lifestyle of $180K+ in San Francisco

**Data-Driven Tip:** Look for "Remote - US" roles from SF/NY companies. Get coastal salaries, keep your cost-of-living low.

---

## Technical Architecture {.smaller}

### Why PySpark for ETL?

:::: {.columns}

::: {.column width="50%"}
**The Problem:**

- 13M rows, 683 MB CSV
- 131 columns (mixed types)
- Single-machine tools too slow
- Memory: 4-8 GB required

**The Solution:**

- PySpark lazy evaluation
- Parallel processing
- Efficient filtering
- Save to Parquet (120 MB)
:::

::: {.column width="50%"}
**Performance Impact:**

| Operation | Time | Benefit |
|-----------|------|---------|
| Load raw CSV | 2-3 min | Distributed reading |
| Clean & filter | 5-10 min | Lazy evaluation |
| Save Parquet | 30 sec | Compressed format |
| **Load Parquet** | **1-2 sec** | **Fast reads** |
| Analysis | 1-3 sec | Optimized queries |

**Result:** Process once, analyze infinitely!
:::

::::

---

## Multivariate Analysis: Variables & Rationale {.smaller}

### What Features Drive Salary?

:::: {.columns}

::: {.column width="50%"}
**Categorical Variables**

1. **Location** (`city_name`)
   - Top 10 cities (SF, NYC, Seattle...)
   - **Encoding**: StringIndexer ‚Üí OneHot
   - **Why**: Cost of living + demand

2. **Industry** (`naics2_name`)
   - Technology, Finance, Healthcare...
   - **Encoding**: StringIndexer ‚Üí OneHot
   - **Why**: Sector pay standards

3. **Job Title** (`title`)
   - Top 10 titles (high cardinality)
   - **Encoding**: StringIndexer ‚Üí OneHot
   - **Why**: Role complexity proxy
:::

::: {.column width="50%"}
**Numerical Variables**

4. **Experience** (`experience_years`)
   - Continuous: 0-30+ years
   - **No transformation**
   - **Why**: Linear relationship expected

5. **Skills** (`skills_count`)
   - Discrete count: 0-20 skills
   - **Derivation**: Text analysis
   - **Why**: Technical breadth premium

**Why One-Hot Encoding?**
- Nominal categories (no order)
- Avoids false ordinal assumptions
- Each city gets independent coefficient
:::

::::

::: {.notes}
This is the multivariate specification - we're predicting salary using multiple predictors simultaneously. One-hot encoding is critical because "San Francisco" isn't "3√ó Boston" - they're different categories.
:::

---

## Machine Learning: Salary Prediction {.smaller}

### How We Built the Model

:::: {.columns}

::: {.column width="50%"}
**Step 1: Prepare the Data**
- Convert categories to numbers
  - Cities ‚Üí numeric codes
  - Industries ‚Üí numeric codes
  - Job titles ‚Üí numeric codes

**Step 2: Combine Features**
- Location + Industry + Title + Experience
- Creates 50+ features automatically

**Step 3: Train the Model**
- Uses PySpark MLlib (handles big data)
- Learns patterns from 72,000 jobs
:::

::: {.column width="50%"}
**üìä Model Performance**

‚úÖ **Accuracy**: 83% of salary variance explained (R¬≤ = 0.83)

‚úÖ **Typical Error**: $17,000 (RMSE)
- On a $114K median salary
- That's 15% error - acceptable!

‚úÖ **Speed**: Predicts instantly
- Processes 72K jobs in seconds

**üéØ What This Means**:
Model can predict your expected salary with ~$17K accuracy based on location, title, industry, and experience.
:::

::::

::: {.notes}
We use machine learning to find patterns in 72,000 real job postings. The model learns what combinations of factors lead to higher or lower salaries.
:::

---

## How the Model Predicts Salary {.smaller}

### What Factors Matter Most?

:::: {.columns}

::: {.column width="50%"}
**The Model Combines Multiple Factors**:

üìç **Location Premium**
- San Francisco: +$25,000
- New York: +$20,000
- Austin: +$10,000

üíº **Job Title Impact**
- Senior Engineer: +$35,000
- Data Scientist: +$30,000
- Product Manager: +$28,000

üè¢ **Industry Bonus**
- Technology: +$15,000
- Finance: +$12,000
- Healthcare: +$8,000
:::

::: {.column width="50%"}
‚è±Ô∏è **Experience Pays**
- Each year: +$3,500
- 7 years = +$24,500

**üéØ Example Prediction**:

Senior Engineer
- Location: San Francisco
- Industry: Technology
- Experience: 7 years

**Predicted Salary**: **$159,500**

*= Baseline $60K + Location $25K + Title $35K + Industry $15K + Experience $24.5K*
:::

::::

::: {.notes}
The model assigns dollar values to each factor and adds them up. This makes it easy to understand what drives higher salaries.
:::

---

## Missing Data Strategy {.smaller}

### The 55% Challenge: No Salary Listed

:::: {.columns}

::: {.column width="50%"}
**The Problem**:
- 55% of job postings don't list salary
- This is market reality (not data error)
- Cannot ignore (biases toward transparent companies)

**Three Approaches**:

1. **Delete Missing** ‚ùå
   - Lose 55% of data
   - Selection bias

2. **Simple Average** ‚ùå
   - Use overall median ($114K) for all missing
   - Problem: Technology pays more than Healthcare
   - Ignores industry differences
:::

::: {.column width="50%"}
3. **Industry-Specific Median** ‚úÖ (Our Choice)

**How It Works**:
- Group jobs by industry first
- Use each industry's median:
  - Technology ‚Üí $125K
  - Healthcare ‚Üí $95K
  - Finance ‚Üí $115K
  - Education ‚Üí $85K

**Why This Works**:
- Respects that industries pay differently
- More accurate than one-size-fits-all
- Fast and interpretable
- Based on domain knowledge

**Result**:
- 100% of jobs have salary estimates
- 55% original + 45% estimated
- Clear documentation in all results
:::

::::

::: {.notes}
This is critical methodological transparency. PySpark's Imputer is limited - it can't do grouped imputation. Future work: multivariate imputation using regression.
:::

---

## Skills That Pay More {.smaller}

### What Skills Should You Learn?

:::: {.columns}

::: {.column width="50%"}
**üî• Most In-Demand Skills:**

1. **Python** - #1 mention
2. **SQL** - #2 mention
3. **AWS** - #3 mention
4. **Machine Learning**
5. **Cloud Platforms**

**üìä How We Found These:**
- Analyzed 72,000 job descriptions
- Used NLP to extract skills
- Counted frequency and salary correlation
:::

::: {.column width="50%"}
**üí∞ Skills with Highest Salary Premium:**

| Skill | Salary Boost |
|-------|-------------|
| Machine Learning | +$15,000 |
| Cloud Certifications | +$12,000 |
| Data Engineering | +$10,000 |
| DevOps | +$9,000 |
| System Design | +$8,000 |

**üéØ Insight**:
Skills in emerging tech (ML, Cloud) command the highest premiums. Traditional skills (Java, SQL) are expected but don't boost salary as much.
:::

::::

---

## Interactive Dashboard Demo {.center}

### Live Quarto Website Features

:::: {.columns}

::: {.column width="50%"}
**Key Pages:**

1. **Salary Insights**
   - Experience progression
   - Education ROI
   - Industry comparisons

2. **Market Dashboard**
   - Geographic analysis
   - Remote work trends
   - Company size effects
:::

::: {.column width="50%"}
**Interactive Features:**

- Plotly hover tooltips
- Zoom & pan charts
- Filter by category
- Export to PNG/SVG

**Performance:**

- Page load: <2 seconds
- Figure cache: Pre-generated
- No server required (static!)
:::

::::

---

## Data Quality & Validation {.smaller}

### Robust Processing Pipeline

```{mermaid}
%%| fig-width: 9
graph TB
    A[Raw Data<br/>13M rows] --> B{Validation}
    B -->|Pass| C[Clean Data]
    B -->|Fail| D[Flag & Log]
    C --> E[Standardize<br/>Column Names]
    E --> F[Impute Missing]
    F --> G[Engineer Features]
    G --> H[Validate Schema]
    H -->|Pass| I[Save Parquet<br/>72K rows]
    H -->|Fail| D

    style B fill:#fff59d
    style C fill:#c8e6c9
    style D fill:#ffcdd2
    style I fill:#b2dfdb
```

**Quality Metrics:**

- Missing salary data: 55.3% (expected for job postings)
- Valid salary range: $20K-$500K (filtered outliers)
- Data retention: 44.7% with complete salary info

---

## Key Technical Decisions {.smaller}

### Architecture Trade-offs

| Decision | Why? | Alternative |
|----------|------|-------------|
| **PySpark for ETL** | 13M rows, distributed processing | Single-machine tools (too slow) |
| **PySpark for Analysis** | Consistent architecture, scalable | Mixed stack (complexity) |
| **PySpark MLlib for ML** | Unified distributed platform | Scikit-learn (not distributed) |
| **Parquet Storage** | Columnar, compressed, fast reads | CSV (slow), HDF5 (not distributed) |
| **Plotly Visualization** | Interactive, web-native | Matplotlib (static only) |
| **Quarto Framework** | Reproducible, Python + Markdown | Jupyter (not for websites) |

**Core Principle:** Choose the right tool for each job!

---

## Challenges & Solutions {.smaller}

:::: {.columns}

::: {.column width="50%"}
### Challenge 1: Memory Issues

**Problem:** 13M rows too large for memory

**Solution:**

- PySpark distributed processing
- Filter to 72K relevant records
- Efficient Parquet storage

### Challenge 2: Slow Renders

**Problem:** Quarto took 5-10 min

**Solution:**

- Pre-generate figures once
- Cache in `figures/` directory
- Smart timestamp checking
:::

::: {.column width="50%"}
### Challenge 3: Column Chaos

**Problem:** UPPERCASE, snake_case mix

**Solution:**

- Standardize to snake_case in ETL
- Centralized column mapping
- Consistent throughout

### Challenge 4: ML Consistency

**Problem:** Need scalable ML for large datasets

**Solution:**

- 100% PySpark MLlib for all ML
- Distributed machine learning
- Unified architecture across ETL and ML
:::

::::

---

## Project Structure {.smaller}

```bash
ad688-scratch/
‚îú‚îÄ‚îÄ data/
‚îÇ ‚îú‚îÄ‚îÄ raw/lightcast_job_postings.csv # 13M rows, 683 MB
‚îÇ ‚îî‚îÄ‚îÄ processed/*.parquet # 72K rows, 120 MB
‚îÇ
‚îú‚îÄ‚îÄ src/ # Python modules
‚îÇ ‚îú‚îÄ‚îÄ data/ # PySpark ETL pipeline
‚îÇ ‚îú‚îÄ‚îÄ analytics/ # PySpark MLlib models
‚îÇ ‚îú‚îÄ‚îÄ ml/ # Advanced ML (regression, classification)
‚îÇ ‚îî‚îÄ‚îÄ visualization/ # Plotly charts
‚îÇ
‚îú‚îÄ‚îÄ notebooks/ # Jupyter analysis
‚îÇ ‚îú‚îÄ‚îÄ data_processing_pipeline_demo.ipynb
‚îÇ ‚îú‚îÄ‚îÄ job_market_skill_analysis.ipynb
‚îÇ ‚îî‚îÄ‚îÄ ml_feature_engineering_lab.ipynb
‚îÇ
‚îú‚îÄ‚îÄ *.qmd # Quarto website pages
‚îú‚îÄ‚îÄ figures/ # Pre-generated charts
‚îî‚îÄ‚îÄ scripts/generate_processed_data.py # One-time preprocessing
```

---

## So What? Actionable Insights for YOU {.center}

### From Data to Decisions

---

## If You're Graduating This Year {.center}

1. **Starting salary:** Target $65K-$75K (realistic baseline)
2. **Geographic strategy:** Remote-first roles = maximum flexibility
3. **Master's decision:** Only if career requires it OR you're passionate
4. **First 3 years:** Focus on learning, not salary optimization

---

## If You're Mid-Career (3-7 years) {.center}

1. **You're in the growth zone:** Biggest ROI on skill development
2. **Job changes:** Strategic moves can accelerate to senior level
3. **Specialization:** Data shows specialists earn 15-25% more
4. **Network:** This is when mentorship matters most

---

## If You're Changing Careers {.center}

1. **Entry data jobs:** $65K starting is normal, even with prior experience
2. **Bootcamps vs. degrees:** Our data shows skills > credentials
3. **Remote opportunities:** Level playing field for career changers

---

## The Bigger Picture {.center}

### What This Research Reveals

---

## Traditional Advice vs. Data {.center}

- "Go to grad school!" ‚Üí **Data says:** Only 20% premium, 12-year breakeven
- "Move to Silicon Valley!" ‚Üí **Data says:** Remote work = geographic arbitrage
- "Get experience first!" ‚Üí **Data says:** True, but strategic job changes accelerate growth

---

## The Real Insight {.center}

Career success isn't about following one path - it's about **understanding the tradeoffs** and making **informed decisions** based on YOUR priorities.

**This platform gives you the data.** You make the decision.

---

## Lessons Learned {.smaller}

### Technical Insights

1. **Unified Architecture**
   - PySpark for entire pipeline (ETL + Analysis + ML)
   - Consistent API across all stages
   - No context switching between tools

2. **Process Once, Use Many Times**
   - 5-10 min preprocessing ‚Üí instant renders
   - Parquet saves 83% storage space
   - Cache figures for 30x speedup

3. **Consistency Matters**
   - 100% PySpark MLlib (not mixed stack)
   - snake_case everywhere
   - Centralized configuration

4. **Documentation is Critical**
   - README, DESIGN, ARCHITECTURE
   - Clear setup instructions
   - Examples in notebooks

---

## Future Enhancements {.smaller}

:::: {.columns}

::: {.column width="50%"}
### Data & Analysis

- **Real-time Updates**
  - Stream new job postings
  - Daily salary trends
  - Emerging skills tracking

- **Advanced ML**
  - Deep learning models
  - Recommendation system
  - Career path prediction

- **More Data Sources**
  - Glassdoor reviews
  - LinkedIn profiles
  - Bureau of Labor Statistics
:::

::: {.column width="50%"}
### Technical Improvements

- **Distributed Deployment**
  - Multi-node Spark cluster
  - Cloud storage (S3)
  - Serverless functions

- **Interactive Dashboard**
  - Streamlit/Dash app
  - User-specific queries
  - Custom filters

- **MLOps Pipeline**
  - Model versioning (MLflow)
  - A/B testing
  - Performance monitoring
:::

::::

---

## Demo: Quick Start {.smaller}

### How to Run This Project

```bash
# 1. Setup environment
git clone https://github.com/samarthya/ad688-scratch
cd ad688-scratch
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# 2. Process data (ONCE - takes 5-10 min)
python scripts/generate_processed_data.py

# 3. Launch website (FAST - <2 seconds per page!)
quarto preview --port 4200
```

**Live Demo:** http://localhost:4200

**Tech Stack:**

- Python 3.11+ | PySpark 4.0.1 | PySpark MLlib
- Plotly 6.3 | Quarto | Parquet

---

## References & Resources {.smaller}

:::: {.columns}

::: {.column width="50%"}
### Documentation

- **README.md** - Project overview
- **DESIGN.md** - Technical implementation
- **ARCHITECTURE.md** - System diagrams
- **SETUP.md** - Environment setup

### Code Repository

**GitHub:** [github.com/samarthya/ad688-scratch](https://github.com/samarthya/ad688-scratch)

**Key Files:**

- `src/data/website_processor.py` - ETL
- `src/analytics/salary_models.py` - ML
- `src/visualization/charts.py` - Plotly
:::

::: {.column width="50%"}
### Technologies

- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [PySpark MLlib Guide](https://spark.apache.org/docs/latest/ml-guide.html)
- [Apache Parquet](https://parquet.apache.org/)
- [Plotly Python](https://plotly.com/python/)
- [Quarto](https://quarto.org/)

### Dataset

**Source:** Lightcast (formerly Emsi Burning Glass)

**Coverage:** 72,000+ job postings, 131 attributes
:::

::::

---

## Thank You! {.center background-color="#1f77b4"}

### Questions?

**Saurabh Sharma**

Boston University | AD688 Final Project

**Project Links:**

- GitHub: [github.com/samarthya/ad688-scratch](https://github.com/samarthya/ad688-scratch)
- Live Demo: http://localhost:4200
- Notebooks: `/notebooks/`

**Key Takeaway:** Data-driven career decisions are now accessible to everyone!

---

